{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n",
      "| id|   name|created_date|\n",
      "+---+-------+------------+\n",
      "|  1| Name_1|  2024-08-02|\n",
      "|  2| Name_2|  2024-08-02|\n",
      "|  3| Name_3|  2024-08-02|\n",
      "|  4| Name_4|  2024-08-02|\n",
      "|  5| Name_5|  2024-08-02|\n",
      "|  6| Name_6|  2024-08-02|\n",
      "|  7| Name_7|  2024-08-02|\n",
      "|  8| Name_8|  2024-08-02|\n",
      "|  9| Name_9|  2024-08-02|\n",
      "| 10|Name_10|  2024-08-02|\n",
      "| 11|Name_11|  2024-08-02|\n",
      "| 12|Name_12|  2024-08-02|\n",
      "| 13|Name_13|  2024-08-02|\n",
      "| 14|Name_14|  2024-08-02|\n",
      "| 15|Name_15|  2024-08-02|\n",
      "| 16|Name_16|  2024-08-02|\n",
      "| 17|Name_17|  2024-08-02|\n",
      "| 18|Name_18|  2024-08-02|\n",
      "| 19|Name_19|  2024-08-02|\n",
      "| 20|Name_20|  2024-08-02|\n",
      "+---+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from GCS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the GCS bucket/file\n",
    "gcs_bucket = \"gs://sovik-big-query-bucket-testing-1/yesterday_records.csv\"\n",
    "\n",
    "# Read the CSV file from GCS into a DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(gcs_bucket)\n",
    "\n",
    "# Show the DataFrame contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/03 17:26:36 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/08/03 17:26:36 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/08/03 17:26:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/08/03 17:26:36 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for today's date: 2024-08-03. Error: [PATH_NOT_FOUND] Path does not exist: gs://sovik-big-query-bucket-testing-1/abc/2024/08/03/*.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or both DataFrames are empty. Skipping further processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n",
      "| id|   name|created_date|\n",
      "+---+-------+------------+\n",
      "|  1| Name_1|  2024-08-03|\n",
      "|  2| Name_2|  2024-08-03|\n",
      "|  3| Name_3|  2024-08-03|\n",
      "|  4| Name_4|  2024-08-03|\n",
      "|  5| Name_5|  2024-08-03|\n",
      "|  6| Name_6|  2024-08-03|\n",
      "|  7| Name_7|  2024-08-03|\n",
      "|  8| Name_8|  2024-08-03|\n",
      "|  9| Name_9|  2024-08-03|\n",
      "| 10|Name_10|  2024-08-03|\n",
      "| 11|Name_11|  2024-08-03|\n",
      "| 12|Name_12|  2024-08-03|\n",
      "| 13|Name_13|  2024-08-03|\n",
      "| 14|Name_14|  2024-08-03|\n",
      "| 15|Name_15|  2024-08-03|\n",
      "| 16|Name_16|  2024-08-03|\n",
      "| 17|Name_17|  2024-08-03|\n",
      "| 18|Name_18|  2024-08-03|\n",
      "| 19|Name_19|  2024-08-03|\n",
      "| 20|Name_20|  2024-08-03|\n",
      "+---+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from GCS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Source path in GCS\n",
    "source_path = \"gs://sovik-big-query-bucket-testing-1/abc\"\n",
    "\n",
    "# Calculate the date today\n",
    "today_date = datetime.now()\n",
    "year1, month1, day1 = today_date.year, today_date.month, today_date.day\n",
    "\n",
    "# Construct the resultant path for today\n",
    "resultant_path = f\"{source_path}/{year1}/{month1:02d}/{day1:02d}/*.csv\"\n",
    "\n",
    "# Initialize empty DataFrame for today\n",
    "df_v1 = None\n",
    "\n",
    "# Try to read today's data from GCS\n",
    "try:\n",
    "    df_v1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(resultant_path)\n",
    "    #df_v1.show()\n",
    "except Exception as e:\n",
    "    print(f\"No files found for today's date: {today_date.strftime('%Y-%m-%d')}. Error: {e}\")\n",
    "\n",
    "# Calculate the date for yesterday\n",
    "yesterday_date = today_date - timedelta(days=1)\n",
    "year0, month0, day0 = yesterday_date.year, yesterday_date.month, yesterday_date.day\n",
    "\n",
    "# Construct the resultant path for yesterday\n",
    "resultant_path1 = f\"{source_path}/{year0}/{month0:02d}/{day0:02d}/*.csv\"\n",
    "\n",
    "# Initialize empty DataFrame for yesterday\n",
    "df_v2 = None\n",
    "\n",
    "# Try to read yesterday's data from GCS\n",
    "try:\n",
    "    df_v2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(resultant_path1)\n",
    "    #df_v2.show()\n",
    "except Exception as e:\n",
    "    print(f\"No files found for yesterday's date: {yesterday_date.strftime('%Y-%m-%d')}. Error: {e}\")\n",
    "\n",
    "# Proceed with your logic only if both DataFrames are not None\n",
    "df_v3=df_v2.withColumn(\"created_date\",lit(today_date.strftime(\"%Y-%m-%d\")))\n",
    "if df_v1 is not None and df_v2 is not None:\n",
    "    # Your logic here\n",
    "    df_v3 = df_v1.union(df_v3).withColumn(\"created_date\", lit(today_date.strftime('%Y-%m-%d')))\n",
    "\n",
    "else:\n",
    "    print(\"One or both DataFrames are empty. Skipping further processing.\")\n",
    "    \n",
    "\n",
    "\n",
    "# Stop the Spark session to ensure proper shutdown\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path=f\"{source_path}/{year1}/{month1:02d}/{day1:02d}/\"\n",
    "df_v3.write.mode(\"overwrite\").csv(path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yesterday_count\n",
    "#today_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 8, 3, 16, 46, 59, 679789)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n",
      "| id|   name|created_date|\n",
      "+---+-------+------------+\n",
      "|  1| Name_1|  2024-08-03|\n",
      "|  2| Name_2|  2024-08-03|\n",
      "|  3| Name_3|  2024-08-03|\n",
      "|  4| Name_4|  2024-08-03|\n",
      "|  5| Name_5|  2024-08-03|\n",
      "|  6| Name_6|  2024-08-03|\n",
      "|  7| Name_7|  2024-08-03|\n",
      "|  8| Name_8|  2024-08-03|\n",
      "|  9| Name_9|  2024-08-03|\n",
      "| 10|Name_10|  2024-08-03|\n",
      "| 11|Name_11|  2024-08-03|\n",
      "| 12|Name_12|  2024-08-03|\n",
      "| 13|Name_13|  2024-08-03|\n",
      "| 14|Name_14|  2024-08-03|\n",
      "| 15|Name_15|  2024-08-03|\n",
      "| 16|Name_16|  2024-08-03|\n",
      "| 17|Name_17|  2024-08-03|\n",
      "| 18|Name_18|  2024-08-03|\n",
      "| 19|Name_19|  2024-08-03|\n",
      "| 20|Name_20|  2024-08-03|\n",
      "+---+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v3=df_v2.withColumn(\"created_date\",lit(today_date.strftime(\"%Y-%m-%d\")))\n",
    "if yesterday_count >0  and today_count>0:   \n",
    "   df_v3=df_v3.union(df_v1)   \n",
    "df_v3.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
