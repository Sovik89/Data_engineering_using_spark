{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.29.4-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Downloading debugpy-1.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Downloading ipython-8.25.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
      "  Downloading jupyter_client-8.6.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting nest-asyncio (from ipykernel)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging (from ipykernel)\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting psutil (from ipykernel)\n",
      "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting pyzmq>=24 (from ipykernel)\n",
      "  Downloading pyzmq-26.0.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting tornado>=6.1 (from ipykernel)\n",
      "  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading prompt_toolkit-3.0.47-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting typing-extensions>=4.6 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from jupyter-client>=6.1.12->ipykernel)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading debugpy-1.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ipython-8.25.0-py3-none-any.whl (817 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.3/817.3 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-8.6.2-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Downloading pyzmq-26.0.3-cp311-cp311-manylinux_2_28_x86_64.whl (919 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.9/919.9 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Downloading prompt_toolkit-3.0.47-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, typing-extensions, traitlets, tornado, six, pyzmq, pygments, psutil, prompt-toolkit, platformdirs, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, python-dateutil, matplotlib-inline, jupyter-core, jedi, comm, asttokens, stack-data, jupyter-client, ipython, ipykernel\n",
      "\u001b[33m  WARNING: The script pygmentize is installed in '/home/sovik/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts jupyter, jupyter-migrate and jupyter-troubleshoot are installed in '/home/sovik/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/home/sovik/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts ipython and ipython3 are installed in '/home/sovik/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.2 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 26.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asttokens-2.4.1 comm-0.2.2 debugpy-1.8.2 decorator-5.1.1 executing-2.0.1 ipykernel-6.29.4 ipython-8.25.0 jedi-0.19.1 jupyter-client-8.6.2 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 packaging-24.1 parso-0.8.4 pexpect-4.9.0 platformdirs-4.2.2 prompt-toolkit-3.0.47 psutil-6.0.0 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.18.0 python-dateutil-2.9.0.post0 pyzmq-26.0.3 six-1.16.0 stack-data-0.6.3 tornado-6.4.1 traitlets-5.14.3 typing-extensions-4.12.2 wcwidth-0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel -U --user --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/26 15:23:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/26 15:23:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/26 15:23:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/26 15:23:01 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.\\\n",
    "    appName(\"Test 4\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(1,5),(2,6),(3,5),(3,6),(1,6)]\n",
    "schema=\"customer_id int,product_key int\"\n",
    "customer_df=spark.createDataFrame(data,schema)\n",
    "\n",
    "\n",
    "data=[(5,),(6,)]\n",
    "schema=\"product_key int\"\n",
    "product_df=spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_products=product_df.count()\n",
    "    \n",
    "count_of_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_v2=customer_df.groupBy(col(\"customer_id\")).\\\n",
    "    agg(countDistinct(col(\"product_key\")).alias(\"count_of_distinct_product\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+\n",
      "|customer_id|count_of_distinct_product|\n",
      "+-----------+-------------------------+\n",
      "|          1|                        2|\n",
      "|          3|                        2|\n",
      "|          2|                        1|\n",
      "+-----------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_v3=customer_df_v2.withColumn(\"all_products\",when((col(\"count_of_distinct_product\")==count_of_products),\"true\").\\\n",
    "    otherwise(\"false\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+------------+\n",
      "|customer_id|count_of_distinct_product|all_products|\n",
      "+-----------+-------------------------+------------+\n",
      "|          1|                        2|        true|\n",
      "|          3|                        2|        true|\n",
      "|          2|                        1|       false|\n",
      "+-----------+-------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df_v3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df_v3.select(col(\"customer_id\")).filter(col(\"all_products\")==\"true\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[('Genece' , 2 , 75000),\n",
    "('𝗝𝗮𝗶𝗺𝗶𝗻' , 2 , 80000 ),\n",
    "('𝗣𝗮𝗻𝗸𝗮𝗷' , 2 , 80000 ),\n",
    "('Tarvares' , 2 , 70000),\n",
    "('Marlania' , 4 , 70000),\n",
    "('Briana' , 4 , 85000),\n",
    "('𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶' , 4 , 55000),\n",
    "('𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮' , 4 , 55000),  \n",
    "('Lakken', 5, 60000),\n",
    "('Latoynia' , 5 , 65000) ]\n",
    "schema=\"emp_name string,dept_id int,salary int\"\n",
    "df=spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------+\n",
      "|          emp_name|dept_id|salary|\n",
      "+------------------+-------+------+\n",
      "|            Genece|      2| 75000|\n",
      "|      𝗝𝗮𝗶𝗺𝗶𝗻|      2| 80000|\n",
      "|      𝗣𝗮𝗻𝗸𝗮𝗷|      2| 80000|\n",
      "|          Tarvares|      2| 70000|\n",
      "|          Marlania|      4| 70000|\n",
      "|            Briana|      4| 85000|\n",
      "|  𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶|      4| 55000|\n",
      "|𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮|      4| 55000|\n",
      "|            Lakken|      5| 60000|\n",
      "|          Latoynia|      5| 65000|\n",
      "+------------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window_spec_max=Window.partitionBy(col(\"dept_id\")).orderBy(col(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2=df.\\\n",
    "    groupBy(col(\"dept_id\")).\\\n",
    "    agg(max(col(\"salary\")).alias(\"max_sal\"),min(col(\"salary\")).alias(\"min_sal\"))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|dept_id|max_sal|min_sal|\n",
      "+-------+-------+-------+\n",
      "|      4|  85000|  55000|\n",
      "|      2|  80000|  70000|\n",
      "|      5|  65000|  60000|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alias = df.alias(\"df\")\n",
    "df_v2_alias = df_v2.alias(\"df_v2\")\n",
    "\n",
    "# Join, filter, and select columns\n",
    "df_v3 = df_alias.join(df_v2_alias, df_alias[\"dept_id\"] == df_v2_alias[\"dept_id\"], how=\"left\") \\\n",
    "    .filter((df_alias[\"salary\"] == df_v2_alias[\"max_sal\"]) | (df_alias[\"salary\"] == df_v2_alias[\"min_sal\"])) \\\n",
    "    .select(df_alias[\"emp_name\"], df_alias[\"dept_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|          emp_name|dept_id|\n",
      "+------------------+-------+\n",
      "|      𝗝𝗮𝗶𝗺𝗶𝗻|      2|\n",
      "|      𝗣𝗮𝗻𝗸𝗮𝗷|      2|\n",
      "|          Tarvares|      2|\n",
      "|            Briana|      4|\n",
      "|  𝗞𝗶𝗺𝗯𝗲𝗿𝗹𝗶|      4|\n",
      "|𝗚𝗮𝗯𝗿𝗶𝗲𝗹𝗹𝗮|      4|\n",
      "|            Lakken|      5|\n",
      "|          Latoynia|      5|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/user/sovik/nag_issues/20240618-095600797.parquet\")\n",
    "#df = spark.read.option(\"wholeFile\", True).option(\"delimiter\", \",\").csv(\"/user/sovik/sample_lqs.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|header__change_seq|header__change_oper|header__change_mask                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |header__stream_position                                               |header__operation|header__transaction_id          |header__timestamp  |header__partition_name         |_id                     |_doc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2.02E34           |I                  |\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00|262;638542149455878970;20240212163654997880|6670081f:00000002:00000000|INSERT           |4155544F434F4D4D4954000000000000|2024-06-17 09:55:43|20240617T093000_20240617T173000|655f0459c250e3e192b75533|{\"_id\":{\"$oid\":\"655f0459c250e3e192b75533\"},\"likely_purchase_date\":{\"$date\":\"2023-11-30T00:00:00.000Z\"},\"status\":\"1668585610133\",\"current_followup_mode\":\"Call\",\"current_followup_mode_id\":\"1668406378088\",\"current_followup_stage\":\"Clarification about request\",\"current_followup_stage_id\":\"1665139043359\",\"email_reminder\":1,\"next_followup_mode\":\"Call\",\"next_followup_mode_id\":\"1668406378088\",\"next_followup_stage\":\"Clarification about request\",\"next_followup_stage_id\":\"1665139043359\",\"auto\":0,\"delete_event_data\":1,\"email_reminder_date\":{\"$date\":\"2023-11-23T13:45:00.000Z\"},\"lead_status\":\"Hot\",\"customer_email\":\"akash.singhavi@adglobal360.com\",\"outlet_id\":\"LQSDEALER1\",\"customer_name\":\"Akash\",\"lead_owner_id\":\"LQSDEALER1-LQSEMP1\",\"lead_lob\":1,\"followup_owner_id\":\"LQSDEALER1-LQSEMP1\",\"followup_owner_name\":\"LQSemp1@sd.com emp1\",\"created_at\":{\"$date\":\"2023-11-23T13:20:49.000Z\"},\"lead_id\":{\"$oid\":\"655eed24c5a9313696981362\"},\"client_id\":450,\"token_number\":\"a26e492c-6f68-4b8b-a462-95b94b6fca96\"}|\n",
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(path=\"/user/sovik/nag_issues/20240618-095600797.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv=spark.read.csv(\"/user/sovik/nag_issues/20240618-095600797.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-----------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|header__change_seq|header__change_oper|header__change_mask                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |header__stream_position                                               |header__operation|header__transaction_id          |header__timestamp      |header__partition_name         |_id                     |_doc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-----------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2.02E34           |I                  |\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00|262;638542149455878970;20240212163654997880|6670081f:00000002:00000000|INSERT           |4155544F434F4D4D4954000000000000|2024-06-17T09:55:43.000|20240617T093000_20240617T173000|655f0459c250e3e192b75533|{\"_id\":{\"$oid\":\"655f0459c250e3e192b75533\"},\"likely_purchase_date\":{\"$date\":\"2023-11-30T00:00:00.000Z\"},\"status\":\"1668585610133\",\"current_followup_mode\":\"Call\",\"current_followup_mode_id\":\"1668406378088\",\"current_followup_stage\":\"Clarification about request\",\"current_followup_stage_id\":\"1665139043359\",\"email_reminder\":1,\"next_followup_mode\":\"Call\",\"next_followup_mode_id\":\"1668406378088\",\"next_followup_stage\":\"Clarification about request\",\"next_followup_stage_id\":\"1665139043359\",\"auto\":0,\"delete_event_data\":1,\"email_reminder_date\":{\"$date\":\"2023-11-23T13:45:00.000Z\"},\"lead_status\":\"Hot\",\"customer_email\":\"akash.singhavi@adglobal360.com\",\"outlet_id\":\"LQSDEALER1\",\"customer_name\":\"Akash\",\"lead_owner_id\":\"LQSDEALER1-LQSEMP1\",\"lead_lob\":1,\"followup_owner_id\":\"LQSDEALER1-LQSEMP1\",\"followup_owner_name\":\"LQSemp1@sd.com emp1\",\"created_at\":{\"$date\":\"2023-11-23T13:20:49.000Z\"},\"lead_id\":{\"$oid\":\"655eed24c5a9313696981362\"},\"client_id\":450,\"token_number\":\"a26e492c-6f68-4b8b-a462-95b94b6fca96\"}|\n",
      "+------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+-----------------+--------------------------------+-----------------------+-------------------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_doc=df_csv.select(col(\"_doc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_doc_json=df_csv_doc.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_value=df_csv_doc_json.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"_doc\":\"{\\\\\"_id\\\\\":{\\\\\"$oid\\\\\":\\\\\"655f0459c250e3e192b75533\\\\\"},\\\\\"likely_purchase_date\\\\\":{\\\\\"$date\\\\\":\\\\\"2023-11-30T00:00:00.000Z\\\\\"},\\\\\"status\\\\\":\\\\\"1668585610133\\\\\",\\\\\"current_followup_mode\\\\\":\\\\\"Call\\\\\",\\\\\"current_followup_mode_id\\\\\":\\\\\"1668406378088\\\\\",\\\\\"current_followup_stage\\\\\":\\\\\"Clarification about request\\\\\",\\\\\"current_followup_stage_id\\\\\":\\\\\"1665139043359\\\\\",\\\\\"email_reminder\\\\\":1,\\\\\"next_followup_mode\\\\\":\\\\\"Call\\\\\",\\\\\"next_followup_mode_id\\\\\":\\\\\"1668406378088\\\\\",\\\\\"next_followup_stage\\\\\":\\\\\"Clarification about request\\\\\",\\\\\"next_followup_stage_id\\\\\":\\\\\"1665139043359\\\\\",\\\\\"auto\\\\\":0,\\\\\"delete_event_data\\\\\":1,\\\\\"email_reminder_date\\\\\":{\\\\\"$date\\\\\":\\\\\"2023-11-23T13:45:00.000Z\\\\\"},\\\\\"lead_status\\\\\":\\\\\"Hot\\\\\",\\\\\"customer_email\\\\\":\\\\\"akash.singhavi@adglobal360.com\\\\\",\\\\\"outlet_id\\\\\":\\\\\"LQSDEALER1\\\\\",\\\\\"customer_name\\\\\":\\\\\"Akash\\\\\",\\\\\"lead_owner_id\\\\\":\\\\\"LQSDEALER1-LQSEMP1\\\\\",\\\\\"lead_lob\\\\\":1,\\\\\"followup_owner_id\\\\\":\\\\\"LQSDEALER1-LQSEMP1\\\\\",\\\\\"followup_owner_name\\\\\":\\\\\"LQSemp1@sd.com emp1\\\\\",\\\\\"created_at\\\\\":{\\\\\"$date\\\\\":\\\\\"2023-11-23T13:20:49.000Z\\\\\"},\\\\\"lead_id\\\\\":{\\\\\"$oid\\\\\":\\\\\"655eed24c5a9313696981362\\\\\"},\\\\\"client_id\\\\\":450,\\\\\"token_number\\\\\":\\\\\"a26e492c-6f68-4b8b-a462-95b94b6fca96\\\\\"}\"}']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [json.loads(json.loads(record)[\"_doc\"].replace('\\\\\"', '\"')) for record in json_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_new_json=spark.read.json(spark.sparkContext.parallelize([cleaned_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- auto: long (nullable = true)\n",
      " |-- client_id: long (nullable = true)\n",
      " |-- created_at: struct (nullable = true)\n",
      " |    |-- $date: string (nullable = true)\n",
      " |-- current_followup_mode: string (nullable = true)\n",
      " |-- current_followup_mode_id: string (nullable = true)\n",
      " |-- current_followup_stage: string (nullable = true)\n",
      " |-- current_followup_stage_id: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- delete_event_data: long (nullable = true)\n",
      " |-- email_reminder: long (nullable = true)\n",
      " |-- email_reminder_date: struct (nullable = true)\n",
      " |    |-- $date: string (nullable = true)\n",
      " |-- followup_owner_id: string (nullable = true)\n",
      " |-- followup_owner_name: string (nullable = true)\n",
      " |-- lead_id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- lead_lob: long (nullable = true)\n",
      " |-- lead_owner_id: string (nullable = true)\n",
      " |-- lead_status: string (nullable = true)\n",
      " |-- likely_purchase_date: struct (nullable = true)\n",
      " |    |-- $date: string (nullable = true)\n",
      " |-- next_followup_mode: string (nullable = true)\n",
      " |-- next_followup_mode_id: string (nullable = true)\n",
      " |-- next_followup_stage: string (nullable = true)\n",
      " |-- next_followup_stage_id: string (nullable = true)\n",
      " |-- outlet_id: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- token_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                $oid|\n",
      "+--------------------+\n",
      "|655f0459c250e3e19...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_json.select([\"_id.$oid\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1= spark.read.parquet(\"/user/sovik/nag_issues/20240621-114659524_bkp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.write.mode(\"overwrite\").csv(path=\"/user/sovik/nag_issues/20240621-114659524_bkp.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2= spark.read.parquet(\"/user/sovik/nag_issues/20240621-114686753.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "|header__change_seq|header__change_oper| header__change_mask|header__stream_position|header__operation|header__transaction_id|  header__timestamp|header__partition_name|                 _id|                _doc|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "|           2.02E34|                  U|\\x03\\x00\\x00\\x00\\...|   245;6385335149205...|           UPDATE|  4155544F434F4D4D4...|2024-06-07 10:04:50|  20240607T093000_2...|655f56e5be6c2c8cb...|{\"_id\":{\"$oid\":\"6...|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.write.mode(\"overwrite\").csv(path=\"/user/sovik/nag_issues/20240621-114686753_v2.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "|header__change_seq|header__change_oper| header__change_mask|header__stream_position|header__operation|header__transaction_id|   header__timestamp|header__partition_name|                 _id|                _doc|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "|           2.02E34|                  U|\\x03\\x00\\x00\\x00\\...|   245;6385335149205...|           UPDATE|  4155544F434F4D4D4...|2024-06-07T10:04:...|  20240607T093000_2...|655f56e5be6c2c8cb...|{\"_id\":{\"$oid\":\"6...|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+--------------------+----------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(path=\"/user/sovik/nag_issues/20240621-114686753_v2.csv\",header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark: 'SparkSession')\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark: 'SparkSession')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a CSV file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  format(self, source: str) -> 'DataFrameReader'\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.format('json')\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.format('json').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.json(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Load a CSV file with format, schema and options specified.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     df = spark.read.load(\n",
      " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
      " |      ...     df.printSchema()\n",
      " |      ...     df.show()\n",
      " |      root\n",
      " |       |-- age: long (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key for the option to set.\n",
      " |      value\n",
      " |          The value for the option to set.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.schema(df.schema).option(\n",
      " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **options : dict\n",
      " |          The dictionary of string keys and prmitive-type values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with a header.\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     spark.read.options(\n",
      " |      ...         nullValue=\"Hyukjin Kwon\",\n",
      " |      ...         header=True\n",
      " |      ...     ).format('csv').load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a ORC file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a ORC file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.orc(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.parquet(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |      <...readwriter.DataFrameReader object ...>\n",
      " |      \n",
      " |      Specify the schema with reading a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
      " |      root\n",
      " |       |-- col0: integer (nullable = true)\n",
      " |       |-- col1: double (nullable = true)\n",
      " |  \n",
      " |  table(self, tableName: str) -> 'DataFrame'\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.createOrReplaceTempView('tblA')\n",
      " |      >>> spark.read.table('tblA').show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  0|\n",
      " |      |  1|\n",
      " |      |  2|\n",
      " |      |  3|\n",
      " |      |  4|\n",
      " |      |  5|\n",
      " |      |  6|\n",
      " |      |  7|\n",
      " |      |  8|\n",
      " |      |  9|\n",
      " |      +---+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      " |  \n",
      " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a text file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a text file\n",
      " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the text file as a DataFrame.\n",
      " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
      " |      +---------+\n",
      " |      |alphabets|\n",
      " |      +---------+\n",
      " |      |        a|\n",
      " |      |        b|\n",
      " |      |        c|\n",
      " |      +---------+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"text\").option(\"header\",True).load(path=\"/user/sovik/nag_issues/sample_lqs_26062024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\"header__change_seq:STRING(35)\",\"header__change_oper:STRING(1)\",\"header__change_mask:BYTES(128)\",\"header__stream_position:STRING(128)\",\"header__operation:STRING(12)\",\"header__transaction_id:STRING(32)\",\"header__timestamp:DATETIME\",\"_id:WSTRING(24)\",\"_doc:NCLOB\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|\"20240626075915000000000000000000029\",\"I\",0180,\"282;638549855603514910;20240212163654997880|667bca53:00000003:00000000\",\"INSERT\",\"4155544F434F4D4D4954000000000000\",2024-06-26 07:59:15.000000,\"667bca5365714e7849d07a2d\",\"{\"\"_id\"\": {\"\"$oid\"\": \"\"667bca5365714e7849d07a2d\"\"}, \"\"comments\"\": \"\"Followup Added\"\", \"\"email_reminder\"\": 1, \"\"email_reminder_date\"\": {\"\"$date\"\": 1719408602000}, \"\"likely_purchase_date\"\": {\"\"$date\"\": 1719964800000}, \"\"delete_event_data\"\": 1, \"\"auto\"\": 0, \"\"status\"\": \"\"1668585610133\"\", \"\"next_followup_mode\"\": \"\"Call\"\", \"\"next_followup_mode_id\"\": \"\"1668406378088\"\", \"\"followup_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"followup_owner_name\"\": \"\"GP User\"\", \"\"lead_status\"\": \"\"Hot\"\", \"\"customer_email\"\": \"\"akash.singhavi@adglobal360.com\"\", \"\"outlet_id\"\": \"\"LQSDEALER1\"\", \"\"customer_name\"\": \"\"Akash\"\", \"\"lead_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"lead_lob\"\": 1, \"\"preferred_date_time\"\": \"\"2024-06-26T13:30:02.000Z\"\", \"\"created_at\"\": {\"\"$date\"\": 1719408555000}, \"\"lead_id\"\": {\"\"$oid\"\": \"\"667bca51b6292c3909ee4e46\"\"}, \"\"followup_added_by_distributor\"\": 1, \"\"client_id\"\": 450}\"|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='\"header__change_seq:STRING(35)\",\"header__change_oper:STRING(1)\",\"header__change_mask:BYTES(128)\",\"header__stream_position:STRING(128)\",\"header__operation:STRING(12)\",\"header__transaction_id:STRING(32)\",\"header__timestamp:DATETIME\",\"_id:WSTRING(24)\",\"_doc:NCLOB\"'),\n",
       " Row(value='\"20240626075915000000000000000000029\",\"I\",0180,\"282;638549855603514910;20240212163654997880|667bca53:00000003:00000000\",\"INSERT\",\"4155544F434F4D4D4954000000000000\",2024-06-26 07:59:15.000000,\"667bca5365714e7849d07a2d\",\"{\"\"_id\"\": {\"\"$oid\"\": \"\"667bca5365714e7849d07a2d\"\"}, \"\"comments\"\": \"\"Followup Added\"\", \"\"email_reminder\"\": 1, \"\"email_reminder_date\"\": {\"\"$date\"\": 1719408602000}, \"\"likely_purchase_date\"\": {\"\"$date\"\": 1719964800000}, \"\"delete_event_data\"\": 1, \"\"auto\"\": 0, \"\"status\"\": \"\"1668585610133\"\", \"\"next_followup_mode\"\": \"\"Call\"\", \"\"next_followup_mode_id\"\": \"\"1668406378088\"\", \"\"followup_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"followup_owner_name\"\": \"\"GP User\"\", \"\"lead_status\"\": \"\"Hot\"\", \"\"customer_email\"\": \"\"akash.singhavi@adglobal360.com\"\", \"\"outlet_id\"\": \"\"LQSDEALER1\"\", \"\"customer_name\"\": \"\"Akash\"\", \"\"lead_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"lead_lob\"\": 1, \"\"preferred_date_time\"\": \"\"2024-06-26T13:30:02.000Z\"\", \"\"created_at\"\": {\"\"$date\"\": 1719408555000}, \"\"lead_id\"\": {\"\"$oid\"\": \"\"667bca51b6292c3909ee4e46\"\"}, \"\"followup_added_by_distributor\"\": 1, \"\"client_id\"\": 450}\"')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header=df_list[0].value\n",
    "df_value=df_list[1].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header_v1=df_header.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"header__change_seq:STRING(35)\"',\n",
       " '\"header__change_oper:STRING(1)\"',\n",
       " '\"header__change_mask:BYTES(128)\"',\n",
       " '\"header__stream_position:STRING(128)\"',\n",
       " '\"header__operation:STRING(12)\"',\n",
       " '\"header__transaction_id:STRING(32)\"',\n",
       " '\"header__timestamp:DATETIME\"',\n",
       " '\"_id:WSTRING(24)\"',\n",
       " '\"_doc:NCLOB\"']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header_doc=df_header_v1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"_doc:NCLOB\"'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header_doc=df_header_doc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_doc:NCLOB\"'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header_doc=df_header_doc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_doc:NCLOB'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"20240626075915000000000000000000029\",\"I\",0180,\"282;638549855603514910;20240212163654997880|667bca53:00000003:00000000\",\"INSERT\",\"4155544F434F4D4D4954000000000000\",2024-06-26 07:59:15.000000,\"667bca5365714e7849d07a2d\",\"{\"\"_id\"\": {\"\"$oid\"\": \"\"667bca5365714e7849d07a2d\"\"}, \"\"comments\"\": \"\"Followup Added\"\", \"\"email_reminder\"\": 1, \"\"email_reminder_date\"\": {\"\"$date\"\": 1719408602000}, \"\"likely_purchase_date\"\": {\"\"$date\"\": 1719964800000}, \"\"delete_event_data\"\": 1, \"\"auto\"\": 0, \"\"status\"\": \"\"1668585610133\"\", \"\"next_followup_mode\"\": \"\"Call\"\", \"\"next_followup_mode_id\"\": \"\"1668406378088\"\", \"\"followup_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"followup_owner_name\"\": \"\"GP User\"\", \"\"lead_status\"\": \"\"Hot\"\", \"\"customer_email\"\": \"\"akash.singhavi@adglobal360.com\"\", \"\"outlet_id\"\": \"\"LQSDEALER1\"\", \"\"customer_name\"\": \"\"Akash\"\", \"\"lead_owner_id\"\": \"\"LQSDEALER1-007007\"\", \"\"lead_lob\"\": 1, \"\"preferred_date_time\"\": \"\"2024-06-26T13:30:02.000Z\"\", \"\"created_at\"\": {\"\"$date\"\": 1719408555000}, \"\"lead_id\"\": {\"\"$oid\"\": \"\"667bca51b6292c3909ee4e46\"\"}, \"\"followup_added_by_distributor\"\": 1, \"\"client_id\"\": 450}\"'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startswith=df_value.find('{\"\"_id\"\"')\n",
    "\n",
    "doc_value=df_value[startswith:]\n",
    "\n",
    "type(doc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_value=doc_value.replace('\"\"','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\": {\"$oid\": \"667bca5365714e7849d07a2d\"}, \"comments\": \"Followup Added\", \"email_reminder\": 1, \"email_reminder_date\": {\"$date\": 1719408602000}, \"likely_purchase_date\": {\"$date\": 1719964800000}, \"delete_event_data\": 1, \"auto\": 0, \"status\": \"1668585610133\", \"next_followup_mode\": \"Call\", \"next_followup_mode_id\": \"1668406378088\", \"followup_owner_id\": \"LQSDEALER1-007007\", \"followup_owner_name\": \"GP User\", \"lead_status\": \"Hot\", \"customer_email\": \"akash.singhavi@adglobal360.com\", \"outlet_id\": \"LQSDEALER1\", \"customer_name\": \"Akash\", \"lead_owner_id\": \"LQSDEALER1-007007\", \"lead_lob\": 1, \"preferred_date_time\": \"2024-06-26T13:30:02.000Z\", \"created_at\": {\"$date\": 1719408555000}, \"lead_id\": {\"$oid\": \"667bca51b6292c3909ee4e46\"}, \"followup_added_by_distributor\": 1, \"client_id\": 450}'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_value=doc_value[:-1]\n",
    "doc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_doc=[json.loads(doc_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': {'$oid': '667bca5365714e7849d07a2d'},\n",
       "  'comments': 'Followup Added',\n",
       "  'email_reminder': 1,\n",
       "  'email_reminder_date': {'$date': 1719408602000},\n",
       "  'likely_purchase_date': {'$date': 1719964800000},\n",
       "  'delete_event_data': 1,\n",
       "  'auto': 0,\n",
       "  'status': '1668585610133',\n",
       "  'next_followup_mode': 'Call',\n",
       "  'next_followup_mode_id': '1668406378088',\n",
       "  'followup_owner_id': 'LQSDEALER1-007007',\n",
       "  'followup_owner_name': 'GP User',\n",
       "  'lead_status': 'Hot',\n",
       "  'customer_email': 'akash.singhavi@adglobal360.com',\n",
       "  'outlet_id': 'LQSDEALER1',\n",
       "  'customer_name': 'Akash',\n",
       "  'lead_owner_id': 'LQSDEALER1-007007',\n",
       "  'lead_lob': 1,\n",
       "  'preferred_date_time': '2024-06-26T13:30:02.000Z',\n",
       "  'created_at': {'$date': 1719408555000},\n",
       "  'lead_id': {'$oid': '667bca51b6292c3909ee4e46'},\n",
       "  'followup_added_by_distributor': 1,\n",
       "  'client_id': 450}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------+--------------+------------------------+------------------------+-----------------+----+-------------+------------------+---------------------+-----------------+-------------------+-----------+------------------------------+----------+-------------+-----------------+--------+------------------------+------------------------+----------------------------------+-----------------------------+---------+\n",
      "|_id                               |comments      |email_reminder|email_reminder_date     |likely_purchase_date    |delete_event_data|auto|status       |next_followup_mode|next_followup_mode_id|followup_owner_id|followup_owner_name|lead_status|customer_email                |outlet_id |customer_name|lead_owner_id    |lead_lob|preferred_date_time     |created_at              |lead_id                           |followup_added_by_distributor|client_id|\n",
      "+----------------------------------+--------------+--------------+------------------------+------------------------+-----------------+----+-------------+------------------+---------------------+-----------------+-------------------+-----------+------------------------------+----------+-------------+-----------------+--------+------------------------+------------------------+----------------------------------+-----------------------------+---------+\n",
      "|{$oid -> 667bca5365714e7849d07a2d}|Followup Added|1             |{$date -> 1719408602000}|{$date -> 1719964800000}|1                |0   |1668585610133|Call              |1668406378088        |LQSDEALER1-007007|GP User            |Hot        |akash.singhavi@adglobal360.com|LQSDEALER1|Akash        |LQSDEALER1-007007|1       |2024-06-26T13:30:02.000Z|{$date -> 1719408555000}|{$oid -> 667bca51b6292c3909ee4e46}|1                            |450      |\n",
      "+----------------------------------+--------------+--------------+------------------------+------------------------+-----------------+----+-------------+------------------+---------------------+-----------------+-------------------+-----------+------------------------------+----------+-------------+-----------------+--------+------------------------+------------------------+----------------------------------+-----------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to Spark DataFrame\n",
    "# Here, we convert each dictionary to a Row object for DataFrame creation\n",
    "rows = [Row(**row) for row in json_doc]\n",
    "doc_df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show DataFrame\n",
    "doc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- email_reminder: long (nullable = true)\n",
      " |-- email_reminder_date: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- likely_purchase_date: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- delete_event_data: long (nullable = true)\n",
      " |-- auto: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- next_followup_mode: string (nullable = true)\n",
      " |-- next_followup_mode_id: string (nullable = true)\n",
      " |-- followup_owner_id: string (nullable = true)\n",
      " |-- followup_owner_name: string (nullable = true)\n",
      " |-- lead_status: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- outlet_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- lead_owner_id: string (nullable = true)\n",
      " |-- lead_lob: long (nullable = true)\n",
      " |-- preferred_date_time: string (nullable = true)\n",
      " |-- created_at: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- lead_id: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- followup_added_by_distributor: long (nullable = true)\n",
      " |-- client_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"text\").option(\"header\",True).load(path=\"/user/sovik/nag_issues/sample_lqs_26062024_v2.csv\")\n",
    "df_list=df.collect()\n",
    "df_header=df_list[0].value\n",
    "df_value=df_list[1].value\n",
    "df_header_v1=df_header.split(\",\")\n",
    "#header name\n",
    "df_header_doc=df_header_v1[-1]\n",
    "df_header_doc=df_header_doc[1:]\n",
    "#header name\n",
    "df_header_doc=df_header_doc[:-1]\n",
    "\n",
    "#doc value\n",
    "startswith=df_value.find('{\"\"_id\"\"')\n",
    "\n",
    "doc_value=df_value[startswith:]\n",
    "\n",
    "doc_value=doc_value.replace('\"\"','\"')\n",
    "doc_value=doc_value[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\": {\"$oid\": \"65700192dd505706d3d62510\"}, \"lead_id\": {\"$oid\": \"65700125dd50572c6ed624f7\"}, \"outlet_id\": \"LQSDEALER1\", \"first_name\": \"Rfr\", \"middle_name\": \"Tty\", \"last_name\": \"Cgh\", \"phone\": \"7009685249\", \"customer_type\": \"Corporate\", \"customer_type_id\": \"1669198541604\", \"model\": \"JIMNY\", \"model_code\": \"JMN\", \"variant_code\": \"03\", \"variant\": \"ZETA\", \"color_code\": \"GG\", \"color\": \"GRANDEUR GREY\", \"vehicle_sale_type_id\": \"2\", \"vehicle_sale_type\": \"Institution\", \"currency_id\": \"1\", \"currency_name\": \"HUF\", \"base_price\": \"0\", \"payment_at_dealership\": true, \"pre_booking_currency_name\": \"HUF\", \"pre_booking_amount\": \"522\", \"pre_booking_currency_id\": \"1\", \"pre_booking_payment_mode\": \"Demand Draft\", \"pre_booking_payment_mode_id\": \"1674541067635\", \"payment_currency_name\": \"HUF\", \"payment_currency_id\": \"1\", \"payment_mode\": \"Cash + Finance\", \"payment_mode_id\": \"101\", \"refrence_number\": \"280\", \"financer_name\": \"TESTPARTY\", \"financer_id\": \"23232323\", \"finance_amount\": \"Yyy\", \"tentative_delivery_date\": {\"$date\": 1702425600000}, \"digital_signature\": \"https://digitalsignatureslink-qa.s3.amazonaws.com/60a71743-7c21-4ad3-a65b-00da02ce9bc6.png\", \"want_to_exchange\": 0, \"representative_name\": \"\", \"company_vat_no\": \"\", \"representative_role_id\": \"1691505638119\", \"representative_role\": \"Other\", \"verified_phone\": \"7009685249\", \"verified_otp\": \"270100\", \"address\": [{\"address_line_1\": \"Tt\", \"address_line_2\": \"Ff\", \"id_proof_1_url\": \"https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_6cf84dfd-6902-4ff2-8b47-db7eb0e4fe81.jpg\", \"id_proof_1_name\": \"\", \"id_proof_1_id\": \"\", \"id_proof_1_value\": \"Fff\", \"id_proof_2_url\": \"https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_9ebe3c84-ce29-4d1e-9531-b0ef0b586fe5.jpg\", \"id_proof_2_name\": \"\", \"id_proof_2_id\": \"\", \"id_proof_2_value\": \"Fff\", \"address_line_3_name\": \"Address Level 1\", \"address_line_4_name\": \"Address Level 2\", \"address_line_5_name\": \"Address Level 3\", \"address_line_6_name\": \"Address Level 4\", \"address_line_7_name\": \"Address Level 5\"}, {\"address_line_1\": \"Tt\", \"address_line_2\": \"Ff\", \"id_proof_1_url\": \"https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_6cf84dfd-6902-4ff2-8b47-db7eb0e4fe81.jpg\", \"id_proof_1_name\": \"\", \"id_proof_1_id\": \"\", \"id_proof_1_value\": \"Fff\", \"id_proof_2_url\": \"https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_9ebe3c84-ce29-4d1e-9531-b0ef0b586fe5.jpg\", \"id_proof_2_name\": \"\", \"id_proof_2_id\": \"\", \"id_proof_2_value\": \"Fff\", \"address_line_3_name\": \"Address Level 1\", \"address_line_4_name\": \"Address Level 2\", \"address_line_5_name\": \"Address Level 3\", \"address_line_6_name\": \"Address Level 4\", \"address_line_7_name\": \"Address Level 5\"}], \"with_car_reservation\": \"0\", \"scrapping_program\": 0, \"scrap_own_car\": 0, \"allied_business_1_name\": \"Extended Warranty\", \"allied_business_1_value\": \"N/A\", \"allied_business_2_name\": \"Accessories\", \"allied_business_2_value\": \"N/A\", \"allied_business_3_name\": \"Insurance\", \"allied_business_3_value\": \"N/A\", \"allied_business_4_name\": \"Road Side Assistance\", \"allied_business_4_value\": \"N/A\", \"allied_business_5_name\": \"MCP\", \"allied_business_5_value\": \"N/A\", \"lob\": 1, \"prebooking_id\": \"PBKLQSDEALER1202300066\", \"submitted_on\": {\"$date\": 1701859050000}, \"last_modified_at_date_time\": {\"$date\": 1702392896000}, \"updated_on\": {\"$date\": 1701864967000}, \"lead_submitted_on\": {\"$date\": 1701858940000}, \"lead_enquiry_id\": \"ENQLQSDEALER1202300410\", \"lead_source_id\": \"1683869724488\", \"lead_source\": \"References\", \"lead_parent_dealer_id\": \"LQSPARENTDEALER\", \"lead_parent_dealer_name\": \"LQS PARENT DEALER\", \"lead_outlet_name\": \"LQS DEALER 1\", \"status\": \"1674458174567\", \"lead_status\": \"Confirmed\", \"client_id\": 450, \"is_deleted\": 0, \"owner_name\": \"LQSemp1\", \"owner_id\": \"LQSDEALER1-LQSEMP1\", \"company_type\": \"\", \"company_vat_number\": \"\", \"company_type_id\": \"\", \"representative_role_other\": \"\", \"booking_date\": {\"$date\": 1702339200000}, \"booking_number\": \"3213\", \"invoice_date\": {\"$date\": 1702512000000}, \"invoice_number\": \"3213\", \"vin_number\": \"3123123\", \"is_performa_done\": 1}'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------+----------+-----------+---------+----------+-------------+----------------+-----+----------+------------+-------+----------+-------------+--------------------+-----------------+-----------+-------------+----------+---------------------+-------------------------+------------------+-----------------------+------------------------+---------------------------+---------------------+-------------------+--------------+---------------+---------------+-------------+-----------+--------------+------------------------+------------------------------------------------------------------------------------------+----------------+-------------------+--------------+----------------------+-------------------+--------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-----------------+-------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+---+----------------------+------------------------+--------------------------+------------------------+------------------------+----------------------+--------------+-----------+---------------------+-----------------------+----------------+-------------+-----------+---------+----------+----------+------------------+------------+------------------+---------------+-------------------------+------------------------+--------------+------------------------+--------------+----------+----------------+\n",
      "|_id                               |lead_id                           |outlet_id |first_name|middle_name|last_name|phone     |customer_type|customer_type_id|model|model_code|variant_code|variant|color_code|color        |vehicle_sale_type_id|vehicle_sale_type|currency_id|currency_name|base_price|payment_at_dealership|pre_booking_currency_name|pre_booking_amount|pre_booking_currency_id|pre_booking_payment_mode|pre_booking_payment_mode_id|payment_currency_name|payment_currency_id|payment_mode  |payment_mode_id|refrence_number|financer_name|financer_id|finance_amount|tentative_delivery_date |digital_signature                                                                         |want_to_exchange|representative_name|company_vat_no|representative_role_id|representative_role|verified_phone|verified_otp|address                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |with_car_reservation|scrapping_program|scrap_own_car|allied_business_1_name|allied_business_1_value|allied_business_2_name|allied_business_2_value|allied_business_3_name|allied_business_3_value|allied_business_4_name|allied_business_4_value|allied_business_5_name|allied_business_5_value|lob|prebooking_id         |submitted_on            |last_modified_at_date_time|updated_on              |lead_submitted_on       |lead_enquiry_id       |lead_source_id|lead_source|lead_parent_dealer_id|lead_parent_dealer_name|lead_outlet_name|status       |lead_status|client_id|is_deleted|owner_name|owner_id          |company_type|company_vat_number|company_type_id|representative_role_other|booking_date            |booking_number|invoice_date            |invoice_number|vin_number|is_performa_done|\n",
      "+----------------------------------+----------------------------------+----------+----------+-----------+---------+----------+-------------+----------------+-----+----------+------------+-------+----------+-------------+--------------------+-----------------+-----------+-------------+----------+---------------------+-------------------------+------------------+-----------------------+------------------------+---------------------------+---------------------+-------------------+--------------+---------------+---------------+-------------+-----------+--------------+------------------------+------------------------------------------------------------------------------------------+----------------+-------------------+--------------+----------------------+-------------------+--------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-----------------+-------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+---+----------------------+------------------------+--------------------------+------------------------+------------------------+----------------------+--------------+-----------+---------------------+-----------------------+----------------+-------------+-----------+---------+----------+----------+------------------+------------+------------------+---------------+-------------------------+------------------------+--------------+------------------------+--------------+----------+----------------+\n",
      "|{$oid -> 65700192dd505706d3d62510}|{$oid -> 65700125dd50572c6ed624f7}|LQSDEALER1|Rfr       |Tty        |Cgh      |7009685249|Corporate    |1669198541604   |JIMNY|JMN       |03          |ZETA   |GG        |GRANDEUR GREY|2                   |Institution      |1          |HUF          |0         |true                 |HUF                      |522               |1                      |Demand Draft            |1674541067635              |HUF                  |1                  |Cash + Finance|101            |280            |TESTPARTY    |23232323   |Yyy           |{$date -> 1702425600000}|https://digitalsignatureslink-qa.s3.amazonaws.com/60a71743-7c21-4ad3-a65b-00da02ce9bc6.png|0               |                   |              |1691505638119         |Other              |7009685249    |270100      |[{address_line_3_name -> Address Level 1, address_line_6_name -> Address Level 4, address_line_5_name -> Address Level 3, id_proof_2_name -> , id_proof_2_url -> https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_9ebe3c84-ce29-4d1e-9531-b0ef0b586fe5.jpg, id_proof_1_name -> , id_proof_1_value -> Fff, address_line_7_name -> Address Level 5, id_proof_2_id -> , address_line_1 -> Tt, id_proof_1_url -> https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_6cf84dfd-6902-4ff2-8b47-db7eb0e4fe81.jpg, id_proof_1_id -> , address_line_2 -> Ff, id_proof_2_value -> Fff, address_line_4_name -> Address Level 2}, {address_line_3_name -> Address Level 1, address_line_6_name -> Address Level 4, address_line_5_name -> Address Level 3, id_proof_2_name -> , id_proof_2_url -> https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_9ebe3c84-ce29-4d1e-9531-b0ef0b586fe5.jpg, id_proof_1_name -> , id_proof_1_value -> Fff, address_line_7_name -> Address Level 5, id_proof_2_id -> , address_line_1 -> Tt, id_proof_1_url -> https://idproofdocs-qa.s3.amazonaws.com/rn_image_picker_lib_temp_6cf84dfd-6902-4ff2-8b47-db7eb0e4fe81.jpg, id_proof_1_id -> , address_line_2 -> Ff, id_proof_2_value -> Fff, address_line_4_name -> Address Level 2}]|0                   |0                |0            |Extended Warranty     |N/A                    |Accessories           |N/A                    |Insurance             |N/A                    |Road Side Assistance  |N/A                    |MCP                   |N/A                    |1  |PBKLQSDEALER1202300066|{$date -> 1701859050000}|{$date -> 1702392896000}  |{$date -> 1701864967000}|{$date -> 1701858940000}|ENQLQSDEALER1202300410|1683869724488 |References |LQSPARENTDEALER      |LQS PARENT DEALER      |LQS DEALER 1    |1674458174567|Confirmed  |450      |0         |LQSemp1   |LQSDEALER1-LQSEMP1|            |                  |               |                         |{$date -> 1702339200000}|3213          |{$date -> 1702512000000}|3213          |3123123   |1               |\n",
      "+----------------------------------+----------------------------------+----------+----------+-----------+---------+----------+-------------+----------------+-----+----------+------------+-------+----------+-------------+--------------------+-----------------+-----------+-------------+----------+---------------------+-------------------------+------------------+-----------------------+------------------------+---------------------------+---------------------+-------------------+--------------+---------------+---------------+-------------+-----------+--------------+------------------------+------------------------------------------------------------------------------------------+----------------+-------------------+--------------+----------------------+-------------------+--------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-----------------+-------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+---+----------------------+------------------------+--------------------------+------------------------+------------------------+----------------------+--------------+-----------+---------------------+-----------------------+----------------+-------------+-----------+---------+----------+----------+------------------+------------+------------------+---------------+-------------------------+------------------------+--------------+------------------------+--------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_doc=[json.loads(doc_value)]\n",
    "\n",
    "# Convert data to Spark DataFrame\n",
    "# Here, we convert each dictionary to a Row object for DataFrame creation\n",
    "rows = [Row(**row) for row in json_doc]\n",
    "doc_df = spark.createDataFrame(rows)\n",
    "\n",
    "# Show DataFrame\n",
    "doc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                $oid|\n",
      "+--------------------+\n",
      "|65700125dd50572c6...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.select(['lead_id.$oid']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
