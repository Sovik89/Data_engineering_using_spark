{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/26 15:21:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/26 15:21:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/26 15:21:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/26 15:21:17 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.\\\n",
    "    appName(\"Test 3\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\"\n",
    "file_path = \"/tmp/chipotle.tsv\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1=spark.read.\\\n",
    "    format(\"csv\").\\\n",
    "        option(\"header\",\"true\").\\\n",
    "            option(\"inferschema\",\"true\").\\\n",
    "                option(\"sep\",\"\\t\").\\\n",
    "                    load(\"/user/sovik/chipotle.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 10 datas\n",
    "\n",
    "pd1.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of count\n",
    "\n",
    "pd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of columns\n",
    "len(pd1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check index\n",
    "# Add an index column\n",
    "pd1_with_index = pd1.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Show the DataFrame schema\n",
    "pd1_with_index.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows\n",
    "pd1_with_index.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2=pd1.\\\n",
    "    groupby(col(\"item_name\")).\\\n",
    "    agg(count(col(\"quantity\")).alias(\"count_of_popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2_inter=pd2.orderBy(col(\"count_of_popularity\").desc()).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2_inter.select(\"item_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = pd2_inter.collect()\n",
    "\n",
    "# Convert the list of Row objects to a list of dictionaries\n",
    "data_dicts = [row.asDict() for row in data_list]\n",
    "\n",
    "# Convert the list of dictionaries to a JSON string\n",
    "json_data = json.dumps(data_dicts, indent=2)\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0][\"item_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1.select(col(\"item_name\"),col(\"choice_description\")).filter(col(\"item_name\")==data_dicts[0][\"item_name\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = 0.0\n",
    "pd1_revised=pd1.withColumn(\"item_price_revised\",when(cast(\"\"FloatType()).isNull(), default_value)\\\n",
    "                              .otherwise(col(\"item_price\").cast(FloatType()))).drop(\"item_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1_revised.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1_revised.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user\"\n",
    "file_path = \"/tmp/users.tsv\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with header and inferSchema options\n",
    "path = '/user/sovik/practice/users.tsv'\n",
    "users = spark.read.csv(path=path, header=True, inferSchema=True, sep='|')\n",
    "\n",
    "# Add an index column\n",
    "users_with_index = users.withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_val=users_with_index.count() -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.offset(offset_val).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_with_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.select(col(\"index\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.select(col(\"occupation\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.groupBy(col(\"occupation\")).\\\n",
    "    agg(count(col(\"occupation\")).alias(\"count_of_occupation\")).orderBy(col(\"count_of_occupation\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_index.agg(round((sum(col(\"age\"))/count(\"*\")),2).alias(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=users_with_index.groupBy(col(\"occupation\")).\\\n",
    "    agg(count(col(\"occupation\")).alias(\"count_of_occupation\")).orderBy(col(\"count_of_occupation\").desc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min=df.select(min(col(\"count_of_occupation\")).alias(\"min_val\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the result to get it as a list of Row objects\n",
    "min_val_row = df_min.collect()\n",
    "\n",
    "# Extract the minimum value from the Row object\n",
    "min_val = min_val_row[0]['min_val']\n",
    "\n",
    "# Print the minimum value\n",
    "print(\"Minimum value of count_of_occupation:\", min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(users_with_index.columns[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list\n",
    "d1 = [1, 2, 3, 4, 6, 4]\n",
    "\n",
    "# Create a DataFrame from the list using lit function\n",
    "df_explode = explode(d1)\n",
    "\n",
    "# Explode the DataFrame\n",
    "\n",
    "\n",
    "# Show the exploded DataFrame\n",
    "df_explode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chipotle=spark.read.\\\n",
    "    format(\"csv\").\\\n",
    "        option(\"header\",\"true\").\\\n",
    "            option(\"inferschema\",\"true\").\\\n",
    "                option(\"sep\",\"\\t\").\\\n",
    "                    load(\"/user/sovik/practice/chipotle.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+----------+\n",
      "|order_id|quantity|           item_name|  choice_description|item_price|\n",
      "+--------+--------+--------------------+--------------------+----------+\n",
      "|       1|       1|Chips and Fresh T...|                NULL|    $2.39 |\n",
      "|       1|       1|                Izze|        [Clementine]|    $3.39 |\n",
      "|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |\n",
      "|       1|       1|Chips and Tomatil...|                NULL|    $2.39 |\n",
      "|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
      "|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
      "|       3|       1|       Side of Chips|                NULL|    $1.69 |\n",
      "|       4|       1|       Steak Burrito|[Tomatillo Red Ch...|   $11.75 |\n",
      "|       4|       1|    Steak Soft Tacos|[Tomatillo Green ...|    $9.25 |\n",
      "|       5|       1|       Steak Burrito|[Fresh Tomato Sal...|    $9.25 |\n",
      "|       5|       1| Chips and Guacamole|                NULL|    $4.45 |\n",
      "|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
      "|       7|       1| Chips and Guacamole|                NULL|    $4.45 |\n",
      "|       8|       1|Chips and Tomatil...|                NULL|    $2.39 |\n",
      "|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
      "|       9|       2|         Canned Soda|            [Sprite]|    $2.18 |\n",
      "|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
      "+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chipotle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1294107939.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    chipotle.filter(col(\"item_price\") \"\").count()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "chipotle.filter(col(\"item_price\") \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value=0.0\n",
    "chipotle_dedolarified=chipotle.withColumn(\"item_price_dedolarified\",regexp_replace(col(\"item_price\"),\"^\\$\",\"\"))\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+----------+-----------------------+\n",
      "|order_id|quantity|           item_name|  choice_description|item_price|item_price_dedolarified|\n",
      "+--------+--------+--------------------+--------------------+----------+-----------------------+\n",
      "|       1|       1|Chips and Fresh T...|                NULL|    $2.39 |                  2.39 |\n",
      "|       1|       1|                Izze|        [Clementine]|    $3.39 |                  3.39 |\n",
      "|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |                  3.39 |\n",
      "|       1|       1|Chips and Tomatil...|                NULL|    $2.39 |                  2.39 |\n",
      "|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |                 16.98 |\n",
      "|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |                 10.98 |\n",
      "|       3|       1|       Side of Chips|                NULL|    $1.69 |                  1.69 |\n",
      "|       4|       1|       Steak Burrito|[Tomatillo Red Ch...|   $11.75 |                 11.75 |\n",
      "|       4|       1|    Steak Soft Tacos|[Tomatillo Green ...|    $9.25 |                  9.25 |\n",
      "|       5|       1|       Steak Burrito|[Fresh Tomato Sal...|    $9.25 |                  9.25 |\n",
      "|       5|       1| Chips and Guacamole|                NULL|    $4.45 |                  4.45 |\n",
      "|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |                  8.75 |\n",
      "|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |                  8.75 |\n",
      "|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |                 11.25 |\n",
      "|       7|       1| Chips and Guacamole|                NULL|    $4.45 |                  4.45 |\n",
      "|       8|       1|Chips and Tomatil...|                NULL|    $2.39 |                  2.39 |\n",
      "|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |                  8.49 |\n",
      "|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |                  8.49 |\n",
      "|       9|       2|         Canned Soda|            [Sprite]|    $2.18 |                  2.18 |\n",
      "|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |                  8.75 |\n",
      "+--------+--------+--------------------+--------------------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chipotle_dedolarified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipotle_updated=chipotle_dedolarified.withColumn(\"item_price_floatified\",col(\"item_price_dedolarified\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv1=chipotle_updated.drop(col(\"item_price_dedolarified\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2=dfv1.drop(col(\"item_price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- choice_description: string (nullable = true)\n",
      " |-- item_price_floatified: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{IntegerType(): ['order_id', 'quantity'], StringType(): ['item_name', 'choice_description'], FloatType(): ['item_price_floatified']}\n"
     ]
    }
   ],
   "source": [
    "df_schema=df_v2.schema\n",
    "field_tyles={}\n",
    "\n",
    "if isinstance(df_schema,StructType):\n",
    "\n",
    "    for field in df_schema:\n",
    "        if field.dataType in field_tyles:\n",
    "            field_tyles[field.dataType].append(field.name)\n",
    "        else:\n",
    "            field_tyles[field.dataType]=[field.name]\n",
    "            \n",
    "print(field_tyles)\n",
    "count=0\n",
    "\n",
    "for type,value in field_tyles.items():\n",
    "    df= df_v2.select(*value)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|order_id|quantity|\n",
      "+--------+--------+\n",
      "|       1|       1|\n",
      "|       1|       1|\n",
      "|       1|       1|\n",
      "|       1|       1|\n",
      "|       2|       2|\n",
      "|       3|       1|\n",
      "|       3|       1|\n",
      "|       4|       1|\n",
      "|       4|       1|\n",
      "|       5|       1|\n",
      "|       5|       1|\n",
      "|       6|       1|\n",
      "|       6|       1|\n",
      "|       7|       1|\n",
      "|       7|       1|\n",
      "|       8|       1|\n",
      "|       8|       1|\n",
      "|       9|       1|\n",
      "|       9|       2|\n",
      "|      10|       1|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|           item_name|  choice_description|\n",
      "+--------------------+--------------------+\n",
      "|Chips and Fresh T...|                NULL|\n",
      "|                Izze|        [Clementine]|\n",
      "|    Nantucket Nectar|             [Apple]|\n",
      "|Chips and Tomatil...|                NULL|\n",
      "|        Chicken Bowl|[Tomatillo-Red Ch...|\n",
      "|        Chicken Bowl|[Fresh Tomato Sal...|\n",
      "|       Side of Chips|                NULL|\n",
      "|       Steak Burrito|[Tomatillo Red Ch...|\n",
      "|    Steak Soft Tacos|[Tomatillo Green ...|\n",
      "|       Steak Burrito|[Fresh Tomato Sal...|\n",
      "| Chips and Guacamole|                NULL|\n",
      "|Chicken Crispy Tacos|[Roasted Chili Co...|\n",
      "|  Chicken Soft Tacos|[Roasted Chili Co...|\n",
      "|        Chicken Bowl|[Fresh Tomato Sal...|\n",
      "| Chips and Guacamole|                NULL|\n",
      "|Chips and Tomatil...|                NULL|\n",
      "|     Chicken Burrito|[Tomatillo-Green ...|\n",
      "|     Chicken Burrito|[Fresh Tomato Sal...|\n",
      "|         Canned Soda|            [Sprite]|\n",
      "|        Chicken Bowl|[Tomatillo Red Ch...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------------+\n",
      "|item_price_floatified|\n",
      "+---------------------+\n",
      "|                 2.39|\n",
      "|                 3.39|\n",
      "|                 3.39|\n",
      "|                 2.39|\n",
      "|                16.98|\n",
      "|                10.98|\n",
      "|                 1.69|\n",
      "|                11.75|\n",
      "|                 9.25|\n",
      "|                 9.25|\n",
      "|                 4.45|\n",
      "|                 8.75|\n",
      "|                 8.75|\n",
      "|                11.25|\n",
      "|                 4.45|\n",
      "|                 2.39|\n",
      "|                 8.49|\n",
      "|                 8.49|\n",
      "|                 2.18|\n",
      "|                 8.75|\n",
      "+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2.filter(col(\"item_price_floatified\")>10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2.filter(col(\"item_price_floatified\")<10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2.select(col(\"quantity\")).orderBy(col(\"item_price_floatified\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro=spark.read.format(\"csv\").\\\n",
    "    option(\"header\",\"true\").\\\n",
    "        option(\"inferschema\",\"true\").\\\n",
    "            load(\"/user/sovik/practice/Euro_2012_stats_TEAM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(euro.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disipline=euro.select('Team', 'Yellow Cards', 'Red Cards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disipline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all columns except the last 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euro=euro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=df_euro[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=euro.select(*column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "            'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'],\n",
    "            'deaths': [523, 52, 25, 616, 43, 234, 523, 62, 62, 73, 37, 35],\n",
    "            'battles': [5, 42, 2, 2, 4, 7, 8, 3, 4, 7, 8, 9],\n",
    "            'size': [1045, 957, 1099, 1400, 1592, 1006, 987, 849, 973, 1005, 1099, 1523],\n",
    "            'veterans': [1, 5, 62, 26, 73, 37, 949, 48, 48, 435, 63, 345],\n",
    "            'readiness': [1, 2, 3, 3, 2, 1, 2, 3, 2, 1, 2, 3],\n",
    "            'armored': [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
    "            'deserters': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "            'origin': ['Arizona', 'California', 'Texas', 'Florida', 'Maine', 'Iowa', 'Alaska', 'Washington', 'Oregon', 'Wyoming', 'Louisana', 'Georgia']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=raw_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "army=spark.read.json(spark.sparkContext.parallelize(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "army.regiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = [Row(**{key: value[i] for key, value in raw_data.items()}) for i in range(len(raw_data['regiment']))]\n",
    "\n",
    "# Create DataFrame from list of Rows\n",
    "df = spark.createDataFrame(data_rows)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[('Alice','Badminton,Tennis'),\\\n",
    "    ('Bob','Cricket,Badminton,Tennis'),\\\n",
    "        ('Ralph','Cricket,Badminton,Tennis')]\n",
    "\n",
    "columns=[\"Names\",\"Hobbies\"]\n",
    "\n",
    "people=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_v1=people.withColumn(\"Hobbies_array\",split(col(\"Hobbies\"),\",\")).drop(\"Hobbies\")\n",
    "#people_v2=people.select(col(\"Names\"),explode(col(\"Hobbies\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_v2=people_v1.select(col(\"Names\"),explode(col(\"Hobbies_array\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_city=[('Alice',\"\",\"AP\"),\\\n",
    "    ('','AP',None),\\\n",
    "        (None,'','Bglr')]\n",
    "\n",
    "column_city=[\"City1\",\"City2\",\"City3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city=spark.createDataFrame(data_city,column_city)\n",
    "df_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_v2=df_city.withColumn(\"result_string\",concat_ws(\",\",col(\"City1\"),col(\"City2\"),col(\"City3\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_v3=df_city_v2.withColumn(\"result_array\",split(col(\"result_string\"),\",\")).drop(col(\"result_string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_v3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_vres=df_city_v3.select(explode(col(\"result_array\")).alias(\"result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_vres.select(col(\"result\")).filter(col(\"result\")!=\"\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[(1,\"Steve\"),(2,\"David\"),(3,\"John\"),(4,\"Shree\"),(5,\"Helen\")]\n",
    "data2=[(1,\"SQL\",90),(1,\"PySpark\",100),(2,\"SQL\",70),(2,\"PySpark\",60),(3,\"SQL\",30),(3,\"PySpark\",20),(4,\"SQL\",50),(4,\"PySpark\",50),(5,\"SQL\",45),(5,\"PySpark\",45)]\n",
    "\n",
    "schema1=[\"Id\",\"Name\"]\n",
    "schema2=[\"Id\",\"Subject\",\"Mark\"]\n",
    "\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df2=spark.createDataFrame(data2,schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_subjects=df2.select(col(\"Subject\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined=df1.join(df2,df1[\"Id\"] == df2[\"Id\"],how=\"left\").select(df1[\"Id\"], df1[\"Name\"], df2[\"Subject\"], df2[\"Mark\"])\n",
    "#df_result = df_joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_interim=df_joined.groupBy(col(\"Id\"),col(\"Name\")).agg((sum(col(\"Mark\")/number_of_subjects)).alias(\"Percentage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_interim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_interim.withColumn(\"Grade\",when(col(\"Percentage\")>90,\"DIstinction\").\\\n",
    "                                    when((col(\"Percentage\")<=90) & (col(\"Percentage\")>=50),\"Average\").\\\n",
    "                                            otherwise(\"Fail\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[(1,\"A\",1000,\"IT\"),(2,\"B\",1500,\"IT\"),(3,\"C\",2500,\"IT\"),(4,\"D\",3000,\"HR\"),(5,\"E\",2000,\"HR\"),(6,\"F\",1000,\"HR\")\n",
    "       ,(7,\"G\",4000,\"Sales\"),(8,\"H\",4000,\"Sales\"),(9,\"I\",1000,\"Sales\"),(10,\"J\",2000,\"Sales\")]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"]\n",
    "df=spark.createDataFrame(data1,schema1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec=Window.partitionBy(col(\"DeptName\")).orderBy(col(\"Salary\").desc())\n",
    "\n",
    "df.\\\n",
    "    select(col(\"EmpId\"),col(\"EmpName\"),col(\"Salary\"),col(\"DeptName\")).\\\n",
    "        withColumn(\"rank\",rank().over(windowSpec)).\\\n",
    "            filter(col(\"rank\")==1).\\\n",
    "                orderBy(col(\"Salary\")).\\\n",
    "                    drop(col(\"rank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[(100,\"Raj\",None,1,\"01-04-23\",50000),\n",
    "       (200,\"Joanne\",100,1,\"01-04-23\",4000),(200,\"Joanne\",100,1,\"13-04-23\",4500),(200,\"Joanne\",100,1,\"14-04-23\",4020)]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Mgrid\",\"deptid\",\"salarydt\",\"salary\"]\n",
    "df_salary=spark.createDataFrame(data1,schema1)\n",
    "df_salary.show()\n",
    "#department dataframe\n",
    "data2=[(1,\"IT\"),\n",
    "       (2,\"HR\")]\n",
    "schema2=[\"deptid\",\"deptname\"]\n",
    "df_dept=spark.createDataFrame(data2,schema2)\n",
    "df_dept.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salary_vmanager=df_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined=df_salary.\\\n",
    "    join(df_salary_vmanager,df_salary[\"EmpId\"]==df_salary_vmanager[\"Mgrid\"],how=\"left\").\\\n",
    "        select(df_salary[\"EmpId\"],df_salary[\"EmpName\"],df_salary_vmanager[\"EmpName\"].alias(\"Manager_name\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT current_date()-1 AS current_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str=str(df1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=spark.read.\\\n",
    "    format(\"csv\").\\\n",
    "        option(\"header\",\"true\").\\\n",
    "            option(\"inferschema\",\"true\").\\\n",
    "                option(\"sep\",\"\\t\").\\\n",
    "                    option(\"modifiedbefore\",date_str).\\\n",
    "                    load(\"/user/sovik/practice/chipotle.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+-------+---------+\n",
      "|SOId|    SODate|ItemId|ItemQty|ItemValue|\n",
      "+----+----------+------+-------+---------+\n",
      "|   1|2024-01-01|    I1|     10|     1000|\n",
      "|   2|2024-01-15|    I2|     20|     2000|\n",
      "|   3|2024-02-01|    I3|     10|     1500|\n",
      "|   4|2024-02-15|    I4|     20|     2500|\n",
      "|   5|2024-03-01|    I5|     30|     3000|\n",
      "|   6|2024-03-10|    I6|     40|     3500|\n",
      "|   7|2024-03-20|    I7|     20|     2500|\n",
      "|   8|2024-03-30|    I8|     10|     1000|\n",
      "+----+----------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'2024-01-01',\"I1\",10,1000),(2,\"2024-01-15\",\"I2\",20,2000),(3,\"2024-02-01\",\"I3\",10,1500),(4,\"2024-02-15\",\"I4\",20,2500),(5,\"2024-03-01\",\"I5\",30,3000),(6,\"2024-03-10\",\"I6\",40,3500),(7,\"2024-03-20\",\"I7\",20,2500),(8,\"2024-03-30\",\"I8\",10,1000)]\n",
    "schema=[\"SOId\",\"SODate\",\"ItemId\",\"ItemQty\",\"ItemValue\"]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_v1=df1.withColumn(\"Year_Month\",date_format(col(\"SODate\"),\"MM-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_v2=df1_v1.drop(col(\"SODate\"))\n",
    "df1_v3=df1_v2.groupBy(col(\"Year_Month\")).agg(sum(col(\"ItemValue\")).alias(\"Tot_Sale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec=Window.orderBy(\"Tot_Sale\")\n",
    "\n",
    "df1_v4=df1_v3.withColumn(\"Previous_month_tot_sal\",lag(col(\"Tot_Sale\"),1,0).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_v5=df1_v4.withColumn(\"percrentage_diff\",round(\n",
    "        when(col(\"Previous_month_tot_sal\") !=0, \n",
    "             ((col(\"Tot_Sale\") - col(\"Previous_month_tot_sal\")) * 100) / col(\"Previous_month_tot_sal\"))\n",
    "        .otherwise(\"NA\"), 2\n",
    "    )).drop(col(\"Previous_month_tot_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:18:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/05 17:18:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------+\n",
      "|Year_Month|Tot_Sale|percrentage_diff|\n",
      "+----------+--------+----------------+\n",
      "|   01-2024|    3000|            NULL|\n",
      "|   02-2024|    4000|           33.33|\n",
      "|   03-2024|   10000|           150.0|\n",
      "+----------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1_v5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|Machine_id|processid|activityid|timestamp|\n",
      "+----------+---------+----------+---------+\n",
      "|         0|        0|     start|    0.712|\n",
      "|         0|        0|       end|     1.52|\n",
      "|         0|        1|     start|     3.14|\n",
      "|         0|        1|       end|     4.12|\n",
      "|         1|        0|     start|     0.55|\n",
      "|         1|        0|       end|     1.55|\n",
      "|         1|        1|     start|     0.43|\n",
      "|         1|        1|       end|     1.42|\n",
      "|         2|        0|     start|      4.1|\n",
      "|         2|        0|       end|    4.512|\n",
      "|         2|        1|     start|      2.5|\n",
      "|         2|        1|       end|      5.0|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(0,0,'start',0.712),(0,0,'end',1.520),(0,1,'start',3.140),(0,1,'end',4.120),\n",
    "      (1,0,'start',0.550),(1,0,'end',1.550),(1,1,'start',0.430),(1,1,'end',1.420),\n",
    "      (2,0,'start',4.100),(2,0,'end',4.512),(2,1,'start',2.500),(2,1,'end',5.000)]\n",
    "schema=[\"Machine_id\",\"processid\",\"activityid\",\"timestamp\"]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Machine_id: long (nullable = true)\n",
      " |-- processid: long (nullable = true)\n",
      " |-- activityid: string (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|Machine_id|processid|activityid|timestamp|\n",
      "+----------+---------+----------+---------+\n",
      "|         0|        0|     start|    0.712|\n",
      "|         0|        1|     start|     3.14|\n",
      "|         1|        0|     start|     0.55|\n",
      "|         1|        1|     start|     0.43|\n",
      "|         2|        0|     start|      4.1|\n",
      "|         2|        1|     start|      2.5|\n",
      "+----------+---------+----------+---------+\n",
      "\n",
      "+----------+---------+----------+---------+\n",
      "|Machine_id|processid|activityid|timestamp|\n",
      "+----------+---------+----------+---------+\n",
      "|         0|        0|       end|     1.52|\n",
      "|         0|        1|       end|     4.12|\n",
      "|         1|        0|       end|     1.55|\n",
      "|         1|        1|       end|     1.42|\n",
      "|         2|        0|       end|    4.512|\n",
      "|         2|        1|       end|      5.0|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_start=df1.filter(col(\"activityid\")=='start')\n",
    "df1_end=df1.filter(col(\"activityid\")=='end')\n",
    "\n",
    "df1_start.show()\n",
    "df1_end.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column timestamp#591 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m df1_end\u001b[38;5;241m=\u001b[39mdf1\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivityid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Join the two DataFrames on processid\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df_joined \u001b[38;5;241m=\u001b[39m \u001b[43mdf1_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1_end\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf1_start\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_end\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMachine_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1_end\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1_end\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_timestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1_start\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_timestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the activity run time\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_result \u001b[38;5;241m=\u001b[39m df_joined\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/default/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column timestamp#591 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
     ]
    }
   ],
   "source": [
    "df1_start=df1.filter(col(\"activityid\")=='start')\n",
    "df1_end=df1.filter(col(\"activityid\")=='end')\n",
    "\n",
    "\n",
    "# Join the two DataFrames on processid\n",
    "df_joined = df1_end.join(df1_start, (df1_end[\"processid\"] == df1_start[\"processid\"]) &(df1_end[\"Machine_id\"] == df1_start[\"Machine_id\"]), how=\"inner\") \\\n",
    "    .select(df1_end[\"Machine_id\"], df1_end[\"processid\"], df1_end[\"timestamp\"].alias(\"end_timestamp\"), df1_start[\"timestamp\"].alias(\"start_timestamp\"))\n",
    "\n",
    "# Calculate the activity run time\n",
    "df_result = df_joined.withColumn(\"activity_run\", col(\"end_timestamp\") - col(\"start_timestamp\"))\n",
    "\n",
    "# Show the result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `end` cannot be resolved. Did you mean one of the following? [`Machine_id`, `processid`, `activityid`, `timestamp`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m df1_end \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivityid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Join the two DataFrames on processid\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_joined \u001b[38;5;241m=\u001b[39m df1_end\u001b[38;5;241m.\u001b[39mjoin(df1_start, df1_end[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m df1_start[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessid\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(df1_end[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], df1_end[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessid\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdf1_end\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df1_start[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate the activity run time\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df_result \u001b[38;5;241m=\u001b[39m df_joined\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:3074\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m \n\u001b[1;32m   3004\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;124;03m+---+----+\u001b[39;00m\n\u001b[1;32m   3072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 3074\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Column):\n",
      "File \u001b[0;32m/opt/conda/default/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `end` cannot be resolved. Did you mean one of the following? [`Machine_id`, `processid`, `activityid`, `timestamp`]."
     ]
    }
   ],
   "source": [
    "df1_start = df1.filter(col(\"activityid\") == 'start').alias(\"start\")\n",
    "df1_end = df1.filter(col(\"activityid\") == 'end').alias(\"end\")\n",
    "\n",
    "# Join the two DataFrames on processid\n",
    "df_joined = df1_end.join(df1_start, df1_end[\"processid\"] == df1_start[\"processid\"], how=\"inner\") \\\n",
    "    .select(df1_end[\"Machine_id\"], df1_end[\"processid\"], df1_end[\"end\"], df1_start[\"start\"])\n",
    "\n",
    "# Calculate the activity run time\n",
    "df_result = df_joined.withColumn(\"activity_run\", col(\"end\") - col(\"start\"))\n",
    "\n",
    "# Show the result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-----+\n",
      "|Machine_id|processid|activityid|start|\n",
      "+----------+---------+----------+-----+\n",
      "|         0|        0|     start|0.712|\n",
      "|         0|        1|     start| 3.14|\n",
      "|         1|        0|     start| 0.55|\n",
      "|         1|        1|     start| 0.43|\n",
      "|         2|        0|     start|  4.1|\n",
      "|         2|        1|     start|  2.5|\n",
      "+----------+---------+----------+-----+\n",
      "\n",
      "+----------+---------+----------+-----+\n",
      "|Machine_id|processid|activityid|  end|\n",
      "+----------+---------+----------+-----+\n",
      "|         0|        0|       end| 1.52|\n",
      "|         0|        1|       end| 4.12|\n",
      "|         1|        0|       end| 1.55|\n",
      "|         1|        1|       end| 1.42|\n",
      "|         2|        0|       end|4.512|\n",
      "|         2|        1|       end|  5.0|\n",
      "+----------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_start = df1.filter(col(\"activityid\") == 'start').select(\"*\",col(\"timestamp\").alias(\"start\")).drop(col(\"timestamp\"))\n",
    "df1_end = df1.filter(col(\"activityid\") == 'end').select(\"*\",col(\"timestamp\").alias(\"end\")).drop(col(\"timestamp\"))\n",
    "\n",
    "df1_start.show()\n",
    "df1_end.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------------+\n",
      "|Machine_id|avg(activity_run AS avg_activity_run)|\n",
      "+----------+-------------------------------------+\n",
      "|         0|                                0.894|\n",
      "|         1|                                0.995|\n",
      "|         2|                                1.456|\n",
      "+----------+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join the two DataFrames on processid\n",
    "df_joined = df1_end.join(df1_start, (df1_end[\"processid\"] == df1_start[\"processid\"])&(df1_end[\"Machine_id\"] == df1_start[\"Machine_id\"]), how=\"inner\") \\\n",
    "    .select(df1_end[\"Machine_id\"], df1_end[\"processid\"], df1_end[\"end\"], df1_start[\"start\"])\n",
    "\n",
    "# Calculate the activity run time\n",
    "df_result = df_joined.withColumn(\"activity_run\", col(\"end\") - col(\"start\"))\n",
    "\n",
    "# Show the result\n",
    "df_result.groupBy(col(\"Machine_id\")).agg(avg(col(\"activity_run\").alias(\"avg_activity_run\"))).orderBy(col(\"Machine_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[EmpId: bigint, EmpName: string, Skill: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[(1,'John','ADF'),(1,'John','ADB'),(1,'John','PowerBI'),(2,'Joanne','ADF'),(2,'Joanne','SQL'),(2,'Joanne','Crystal Report'),(3,'Vikas','ADF'),(3,'Vikas','SQL'),(3,'Vikas','SSIS'),(4,'Monu','SQL'),(4,'Monu','SSIS'),(4,'Monu','SSAS'),(4,'Monu','ADF')]\n",
    "schema=[\"EmpId\",\"EmpName\",\"Skill\"]\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+\n",
      "|EmpId|EmpName|         Skill|\n",
      "+-----+-------+--------------+\n",
      "|    1|   John|           ADF|\n",
      "|    1|   John|           ADB|\n",
      "|    1|   John|       PowerBI|\n",
      "|    2| Joanne|           ADF|\n",
      "|    2| Joanne|           SQL|\n",
      "|    2| Joanne|Crystal Report|\n",
      "|    3|  Vikas|           ADF|\n",
      "|    3|  Vikas|           SQL|\n",
      "|    3|  Vikas|          SSIS|\n",
      "|    4|   Monu|           SQL|\n",
      "|    4|   Monu|          SSIS|\n",
      "|    4|   Monu|          SSAS|\n",
      "|    4|   Monu|           ADF|\n",
      "+-----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|EmpName|Skill_list_updt       |\n",
      "+-------+----------------------+\n",
      "|John   |ADF,ADB,PowerBI       |\n",
      "|Joanne |ADF,SQL,Crystal Report|\n",
      "|Monu   |SQL,SSIS,SSAS,ADF     |\n",
      "|Vikas  |ADF,SQL,SSIS          |\n",
      "+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_v1=df1.groupBy(col(\"EmpName\")).agg(collect_list(col(\"Skill\")).alias(\"Skill_list\"))\n",
    "df1_v1.withColumn(\"Skill_list_updt\",concat_ws(',',col(\"Skill_list\"))).drop(col(\"Skill_list\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 30,\n",
    "  \"address\": {\n",
    "    \"street\": \"123 Main St\",\n",
    "    \"city\": \"Anytown\",\n",
    "    \"state\": \"CA\",\n",
    "    \"postalCode\": \"12345\"\n",
    "  },\n",
    "  \"contactDetails\": [\n",
    "    {\n",
    "      \"type\": \"home\",\n",
    "      \"number\": \"555-555-5555\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"work\",\n",
    "      \"number\": \"555-555-5556\"\n",
    "    },\n",
    "    {\n",
    "      \"email\": \"johndoe@example.com\"\n",
    "    }\n",
    "  ],\n",
    "  \"projects\": [\n",
    "    {\n",
    "      \"name\": \"Project A\",\n",
    "      \"deadline\": \"2024-12-31\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Project B\",\n",
    "      \"deadline\": \"2025-06-30\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "json_data=json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize([json_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---+-------------------------------------------------------------------------------------------+--------+--------------------------------------------------+\n",
      "|address                          |age|contactDetails                                                                             |name    |projects                                          |\n",
      "+---------------------------------+---+-------------------------------------------------------------------------------------------+--------+--------------------------------------------------+\n",
      "|{Anytown, 12345, CA, 123 Main St}|30 |[{NULL, 555-555-5555, home}, {NULL, 555-555-5556, work}, {johndoe@example.com, NULL, NULL}]|John Doe|[{2024-12-31, Project A}, {2025-06-30, Project B}]|\n",
      "+---------------------------------+---+-------------------------------------------------------------------------------------------+--------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- postalCode: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- contactDetails: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- number: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- projects: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- deadline: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name,address,projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v1=df.select(col(\"name\"),col(\"address\"),col(\"projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------+--------------------------------------------------+\n",
      "|name    |address                          |projects                                          |\n",
      "+--------+---------------------------------+--------------------------------------------------+\n",
      "|John Doe|{Anytown, 12345, CA, 123 Main St}|[{2024-12-31, Project A}, {2025-06-30, Project B}]|\n",
      "+--------+---------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2=df_v1.select(\"*\").withColumn(\"projects_exploded\",explode(col(\"projects\"))).drop(col(\"projects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- postalCode: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |-- projects_exploded: struct (nullable = true)\n",
      " |    |-- deadline: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v3=df_v2.select(\"*\").withColumn(\"deadline\",df_v2['projects_exploded']['deadline']).drop(col(\"projects_exploded\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- postalCode: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |-- deadline: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=[\n",
    "  {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"address\": {\n",
    "      \"street\": \"123 Main St\",\n",
    "      \"city\": \"Anytown\",\n",
    "      \"state\": \"CA\",\n",
    "      \"postalCode\": \"12345\"\n",
    "    },\n",
    "    \"contactDetails\": [\n",
    "      {\n",
    "        \"type\": \"home\",\n",
    "        \"number\": \"555-555-5555\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"work\",\n",
    "        \"number\": \"555-555-5556\"\n",
    "      },\n",
    "      {\n",
    "        \"email\": \"johndoe@example.com\"\n",
    "      }\n",
    "    ],\n",
    "    \"projects\": [\n",
    "      {\n",
    "        \"name\": \"Project A\",\n",
    "        \"deadline\": \"2024-12-31\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Project B\",\n",
    "        \"deadline\": \"2025-06-30\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"Jane Smith\",\n",
    "    \"age\": 28,\n",
    "    \"address\": {\n",
    "      \"street\": \"456 Elm St\",\n",
    "      \"city\": \"Othertown\",\n",
    "      \"state\": \"NY\",\n",
    "      \"postalCode\": \"67890\"\n",
    "    },\n",
    "    \"contactDetails\": [\n",
    "      {\n",
    "        \"type\": \"home\",\n",
    "        \"number\": \"555-555-1234\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"work\",\n",
    "        \"number\": \"555-555-5678\"\n",
    "      },\n",
    "      {\n",
    "        \"email\": \"janesmith@example.com\"\n",
    "      }\n",
    "    ],\n",
    "    \"projects\": [\n",
    "      {\n",
    "        \"name\": \"Project C\",\n",
    "        \"deadline\": \"2025-03-15\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Project D\",\n",
    "        \"deadline\": \"2025-08-22\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "json_data2 = json.dumps(data2)\n",
    "\n",
    "# Create a DataFrame from the JSON string\n",
    "df2 = spark.read.json(spark.sparkContext.parallelize([json_data2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+----------+--------------------+\n",
      "|             address|age|      contactDetails|      name|            projects|\n",
      "+--------------------+---+--------------------+----------+--------------------+\n",
      "|{Anytown, 12345, ...| 30|[{NULL, 555-555-5...|  John Doe|[{2024-12-31, Pro...|\n",
      "|{Othertown, 67890...| 28|[{NULL, 555-555-1...|Jane Smith|[{2025-03-15, Pro...|\n",
      "+--------------------+---+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_c2r=spark.read.csv(path='/user/sovik/practice/col_2_row.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "|             svoc_id|        token_number|dealer_code|lob|transaction_no|transaction_type_booking|transaction_type_invoice|transaction_type_delivery|transaction_type_job_card|\n",
      "+--------------------+--------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "|2072e38e-192f-49a...|23d89455-30eb-4d3...| LQSDEALER1|  1|   OB202300005|                 booking|                 invoice|             deliverynote|                 job_card|\n",
      "|61fd04b4-f6bf-474...|d0e52e2d-867b-43b...| LQSDEALER1|  1|   OB202300005|                 booking|                 invoice|             deliverynote|                 job_card|\n",
      "|933d804a-b966-427...|d0e52e2d-867b-43b...| LQSDEALER1|  1|   OB202300005|                 booking|                 invoice|             deliverynote|                 job_card|\n",
      "|c40f4990-3a7b-45b...|23d89455-30eb-4d3...| LQSDEALER1|  1|   OB202300005|                 booking|                 invoice|             deliverynote|                 job_card|\n",
      "+--------------------+--------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c2r.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT svoc_id,token_number,dealer_code,lob,transaction_no,transaction_type FROM (SELECT svoc_id,token_number,dealer_code,lob,transaction_no,stack(4,'transaction_type_booking', transaction_type_booking,'transaction_type_invoice', transaction_type_invoice,'transaction_type_delivery', transaction_type_delivery,'transaction_type_jobcart', transaction_type_jobcart) AS (type_category, transaction_type) FROM pivot_transaction_type) AS unpiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c2r.createOrReplaceTempView(\"transition_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.current_database() -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases:\n",
      "default\n",
      "nyse_db\n",
      "retail_db\n"
     ]
    }
   ],
   "source": [
    "databases = spark.catalog.listDatabases()\n",
    "print(\"Databases:\")\n",
    "for db in databases:\n",
    "    print(db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use retail_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[current_database(): string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c2r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Database after change: retail_db\n"
     ]
    }
   ],
   "source": [
    "current_database = spark.catalog.currentDatabase()\n",
    "print(f\"Current Database after change: {current_database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c2r.createOrReplaceTempView(\"transition_type_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "|svoc_id                             |token_number                        |dealer_code|lob|transaction_no|transaction_type_booking|transaction_type_invoice|transaction_type_delivery|transaction_type_job_card|\n",
      "+------------------------------------+------------------------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "|2072e38e-192f-49a6-ac7b-64fec86b5ac3|23d89455-30eb-4d3c-b492-5f1545679d02|LQSDEALER1 |1  |OB202300005   |booking                 |invoice                 |deliverynote             |job_card                 |\n",
      "|61fd04b4-f6bf-474a-8ddc-620fd73f2577|d0e52e2d-867b-43bc-ae59-5dc370144ed7|LQSDEALER1 |1  |OB202300005   |booking                 |invoice                 |deliverynote             |job_card                 |\n",
      "|933d804a-b966-427f-8105-119c19ab5dc9|d0e52e2d-867b-43bc-ae59-5dc370144ed7|LQSDEALER1 |1  |OB202300005   |booking                 |invoice                 |deliverynote             |job_card                 |\n",
      "|c40f4990-3a7b-45bf-b939-e9f96c755cc4|23d89455-30eb-4d3c-b492-5f1545679d02|LQSDEALER1 |1  |OB202300005   |booking                 |invoice                 |deliverynote             |job_card                 |\n",
      "+------------------------------------+------------------------------------+-----------+---+--------------+------------------------+------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transition_type_1\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `transaction_type_jobcart` cannot be resolved. Did you mean one of the following? [`transaction_type_job_card`, `transaction_type_booking`, `transaction_type_invoice`, `transaction_type_delivery`, `transaction_no`].; line 2 pos 348;\n'Project ['svoc_id, 'token_number, 'dealer_code, 'lob, 'transaction_no, 'transaction_type]\n+- 'SubqueryAlias unpiv\n   +- 'Project [svoc_id#255, token_number#256, dealer_code#257, lob#258, transaction_no#259, 'stack(4, transaction_type_booking, transaction_type_booking#260, transaction_type_invoice, transaction_type_invoice#261, transaction_type_delivery, transaction_type_delivery#262, transaction_type_jobcart, 'transaction_type_jobcart) AS (type_category, transaction_type)]\n      +- SubqueryAlias transition_type_1\n         +- View (`transition_type_1`, [svoc_id#255,token_number#256,dealer_code#257,lob#258,transaction_no#259,transaction_type_booking#260,transaction_type_invoice#261,transaction_type_delivery#262,transaction_type_job_card#263])\n            +- Relation [svoc_id#255,token_number#256,dealer_code#257,lob#258,transaction_no#259,transaction_type_booking#260,transaction_type_invoice#261,transaction_type_delivery#262,transaction_type_job_card#263] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m          SELECT svoc_id,token_number,dealer_code,lob,transaction_no,transaction_type FROM (SELECT svoc_id,token_number,dealer_code,lob,transaction_no,stack(4,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_type_booking\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, transaction_type_booking,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_type_invoice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, transaction_type_invoice,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_type_delivery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, transaction_type_delivery,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_type_jobcart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, transaction_type_jobcart) AS (type_category, transaction_type) FROM transition_type_1) AS unpiv         \u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m          \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/default/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `transaction_type_jobcart` cannot be resolved. Did you mean one of the following? [`transaction_type_job_card`, `transaction_type_booking`, `transaction_type_invoice`, `transaction_type_delivery`, `transaction_no`].; line 2 pos 348;\n'Project ['svoc_id, 'token_number, 'dealer_code, 'lob, 'transaction_no, 'transaction_type]\n+- 'SubqueryAlias unpiv\n   +- 'Project [svoc_id#255, token_number#256, dealer_code#257, lob#258, transaction_no#259, 'stack(4, transaction_type_booking, transaction_type_booking#260, transaction_type_invoice, transaction_type_invoice#261, transaction_type_delivery, transaction_type_delivery#262, transaction_type_jobcart, 'transaction_type_jobcart) AS (type_category, transaction_type)]\n      +- SubqueryAlias transition_type_1\n         +- View (`transition_type_1`, [svoc_id#255,token_number#256,dealer_code#257,lob#258,transaction_no#259,transaction_type_booking#260,transaction_type_invoice#261,transaction_type_delivery#262,transaction_type_job_card#263])\n            +- Relation [svoc_id#255,token_number#256,dealer_code#257,lob#258,transaction_no#259,transaction_type_booking#260,transaction_type_invoice#261,transaction_type_delivery#262,transaction_type_job_card#263] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT svoc_id,token_number,dealer_code,lob,transaction_no,transaction_type FROM (SELECT svoc_id,token_number,dealer_code,lob,transaction_no,stack(4,'transaction_type_booking', transaction_type_booking,'transaction_type_invoice', transaction_type_invoice,'transaction_type_delivery', transaction_type_delivery,'transaction_type_jobcart', transaction_type_jobcart) AS (type_category, transaction_type) FROM transition_type_1) AS unpiv         \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pd1=spark.read.\\\n",
    "    parquet(\"/user/sovik/nag_issues/20240608-114546125.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "|header__change_seq|header__change_oper| header__change_mask|header__stream_position|header__operation|header__transaction_id|  header__timestamp|header__partition_name|                 _id|                _doc|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "|2.0240607114526E34|                  I|\\x03\\x00\\x00\\x00\\...|   245;6385335753204...|           INSERT|  4155544F434F4D4D4...|2024-06-07 11:45:26|  20240607T093000_2...|6662f2d6ed500c821...|{\"_id\": {\"$oid\": ...|\n",
      "+------------------+-------------------+--------------------+-----------------------+-----------------+----------------------+-------------------+----------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1_schema=pd1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('header__change_seq', DoubleType(), True), StructField('header__change_oper', StringType(), True), StructField('header__change_mask', StringType(), True), StructField('header__stream_position', StringType(), True), StructField('header__operation', StringType(), True), StructField('header__transaction_id', StringType(), True), StructField('header__timestamp', TimestampNTZType(), True), StructField('header__partition_name', StringType(), True), StructField('_id', StringType(), True), StructField('_doc', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd1_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list=list(pd1_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_col=schema_list[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_doc'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json=pd1.select(col(last_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                _doc|\n",
      "+--------------------+\n",
      "|{\"_id\": {\"$oid\": ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_value=df_json.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_value=json_value.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"_doc\":\"{\\\\\"_id\\\\\": {\\\\\"$oid\\\\\": \\\\\"6662f2d6ed500c821fd0f877\\\\\"}, \\\\\"outlet_id\\\\\": \\\\\"002\\\\\", \\\\\"Email\\\\\": \\\\\"sagar.daware@suzuki-digital.com\\\\\", \\\\\"first_name\\\\\": \\\\\"Sagar \\\\\", \\\\\"last_name\\\\\": \\\\\"Daware\\\\\", \\\\\"communication_medium_id\\\\\": \\\\\"1663751859054\\\\\", \\\\\"Phone\\\\\": \\\\\"8888657594\\\\\", \\\\\"lead_source_id\\\\\": \\\\\"1663926070773\\\\\", \\\\\"lead_source\\\\\": \\\\\"Website\\\\\", \\\\\"lead_mode_id\\\\\": \\\\\"1710746578356\\\\\", \\\\\"lead_mode\\\\\": \\\\\"Test Drive Form\\\\\", \\\\\"outlet_name\\\\\": \\\\\"RANA SUZUKI\\\\\", \\\\\"client_id\\\\\": 450, \\\\\"parent_dealer_id\\\\\": \\\\\"LQS-ADMIN\\\\\", \\\\\"parent_dealer_name\\\\\": \\\\\"LQS-ADMIN\\\\\", \\\\\"distributor_code\\\\\": \\\\\"DIS001\\\\\", \\\\\"outlet_code\\\\\": \\\\\"002\\\\\", \\\\\"is_deleted\\\\\": 0, \\\\\"qualified\\\\\": 1, \\\\\"auto_qualified\\\\\": 1, \\\\\"duplicate\\\\\": 1, \\\\\"different_channel\\\\\": 0, \\\\\"lead_type\\\\\": \\\\\"Web\\\\\", \\\\\"lead_type_id\\\\\": \\\\\"1663926070773\\\\\", \\\\\"utm_source\\\\\": \\\\\"direct\\\\\", \\\\\"utm_source_id\\\\\": \\\\\"1717588141715\\\\\", \\\\\"utm_medium\\\\\": \\\\\"\\\\\", \\\\\"lead_status\\\\\": \\\\\"Cold\\\\\", \\\\\"status\\\\\": \\\\\"1663751808461\\\\\", \\\\\"form_name\\\\\": \\\\\"Test Drive Form\\\\\", \\\\\"planning_to_purchase_in\\\\\": \\\\\"31\\\\\", \\\\\"email_reminder_date\\\\\": [\\\\\"2024-06-07T17:16:21.000Z\\\\\"], \\\\\"next_followup_stage_id\\\\\": \\\\\"16637385418314\\\\\", \\\\\"next_followup_stage\\\\\": \\\\\"Test Drive\\\\\", \\\\\"next_followup_mode_id\\\\\": \\\\\"1668406378088\\\\\", \\\\\"next_followup_mode\\\\\": \\\\\"Call\\\\\", \\\\\"preferred_date_time\\\\\": \\\\\"2024-06-08T09:00:00.000Z\\\\\", \\\\\"consent_created_using\\\\\": \\\\\"Phone\\\\\", \\\\\"lead_source_category_id\\\\\": \\\\\"1694575409420\\\\\", \\\\\"lead_source_category\\\\\": \\\\\"online\\\\\", \\\\\"lob\\\\\": 1, \\\\\"user_key\\\\\": \\\\\"7f9c2d41466d2e6a004666347e8274f9cedd83aa329534823bf2f67c52a10a045b75b8902cc5a6fee0b2a5bcca3e212daa937dc8889943c4d759776746450971\\\\\", \\\\\"purchase_type\\\\\": \\\\\"counted\\\\\", \\\\\"business_type\\\\\": \\\\\"Sales\\\\\", \\\\\"business_id\\\\\": \\\\\"1663754841597\\\\\", \\\\\"model_code\\\\\": \\\\\"CZ\\\\\", \\\\\"model\\\\\": \\\\\"CIAZ\\\\\", \\\\\"variant_code\\\\\": \\\\\"HYBRID-DELTA\\\\\", \\\\\"variant\\\\\": \\\\\"HYBRID DELTA\\\\\", \\\\\"enquiry_id\\\\\": \\\\\"ENQ002202400052\\\\\", \\\\\"is_quotation_done\\\\\": 0, \\\\\"is_show_room_visit_done\\\\\": 0, \\\\\"is_home_visit_done\\\\\": 0, \\\\\"is_prebooking_done\\\\\": 0, \\\\\"is_testdrive_done\\\\\": 0, \\\\\"likely_purchase_date\\\\\": {\\\\\"$date\\\\\": 1720396800000}, \\\\\"qualified_at\\\\\": {\\\\\"$date\\\\\": 1717780526000}, \\\\\"submitted_on\\\\\": {\\\\\"$date\\\\\": 1717780526000}, \\\\\"last_modified_at_date_time\\\\\": {\\\\\"$date\\\\\": 1717780526000}}\"}']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [json.loads(json.loads(record)[last_col].replace('\\\\\"', '\"')) for record in json_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleansed_last_col=spark.read.json(spark.sparkContext.parallelize([cleaned_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- auto_qualified: long (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- business_type: string (nullable = true)\n",
      " |-- client_id: long (nullable = true)\n",
      " |-- communication_medium_id: string (nullable = true)\n",
      " |-- consent_created_using: string (nullable = true)\n",
      " |-- different_channel: long (nullable = true)\n",
      " |-- distributor_code: string (nullable = true)\n",
      " |-- duplicate: long (nullable = true)\n",
      " |-- email_reminder_date: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- enquiry_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- form_name: string (nullable = true)\n",
      " |-- is_deleted: long (nullable = true)\n",
      " |-- is_home_visit_done: long (nullable = true)\n",
      " |-- is_prebooking_done: long (nullable = true)\n",
      " |-- is_quotation_done: long (nullable = true)\n",
      " |-- is_show_room_visit_done: long (nullable = true)\n",
      " |-- is_testdrive_done: long (nullable = true)\n",
      " |-- last_modified_at_date_time: struct (nullable = true)\n",
      " |    |-- $date: long (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- lead_mode: string (nullable = true)\n",
      " |-- lead_mode_id: string (nullable = true)\n",
      " |-- lead_source: string (nullable = true)\n",
      " |-- lead_source_category: string (nullable = true)\n",
      " |-- lead_source_category_id: string (nullable = true)\n",
      " |-- lead_source_id: string (nullable = true)\n",
      " |-- lead_status: string (nullable = true)\n",
      " |-- lead_type: string (nullable = true)\n",
      " |-- lead_type_id: string (nullable = true)\n",
      " |-- likely_purchase_date: struct (nullable = true)\n",
      " |    |-- $date: long (nullable = true)\n",
      " |-- lob: long (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- model_code: string (nullable = true)\n",
      " |-- next_followup_mode: string (nullable = true)\n",
      " |-- next_followup_mode_id: string (nullable = true)\n",
      " |-- next_followup_stage: string (nullable = true)\n",
      " |-- next_followup_stage_id: string (nullable = true)\n",
      " |-- outlet_code: string (nullable = true)\n",
      " |-- outlet_id: string (nullable = true)\n",
      " |-- outlet_name: string (nullable = true)\n",
      " |-- parent_dealer_id: string (nullable = true)\n",
      " |-- parent_dealer_name: string (nullable = true)\n",
      " |-- planning_to_purchase_in: string (nullable = true)\n",
      " |-- preferred_date_time: string (nullable = true)\n",
      " |-- purchase_type: string (nullable = true)\n",
      " |-- qualified: long (nullable = true)\n",
      " |-- qualified_at: struct (nullable = true)\n",
      " |    |-- $date: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- submitted_on: struct (nullable = true)\n",
      " |    |-- $date: long (nullable = true)\n",
      " |-- user_key: string (nullable = true)\n",
      " |-- utm_medium: string (nullable = true)\n",
      " |-- utm_source: string (nullable = true)\n",
      " |-- utm_source_id: string (nullable = true)\n",
      " |-- variant: string (nullable = true)\n",
      " |-- variant_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleansed_last_col.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
